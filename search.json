[
  {
    "objectID": "00-poster.html",
    "href": "00-poster.html",
    "title": "形式化数学与 Lean 4 定理证明入门",
    "section": "",
    "text": "形式化数学是将数学定义、定理和证明转化为计算机可验证的精确形式的过程。可以认为，数学形式化 = 编写程序，而正确的证明 = 代码通过编译。严谨性是数学研究的基石，形式化数学通过严格的逻辑框架和程序语言确保数学结论的正确性和可复用性，在现代数学体系日渐庞大复杂的背景下具有重要意义。\nLean 4 是一个专为形式化数学设计的编程语言和证明助手，支持数学家将数学定理转化为计算机可验证的代码。它具备高效的编译器和灵活的类型系统，适合构建复杂的数学证明；其数学库 Mathlib 正在飞速扩展，目前已覆盖本科数学大部分内容。Lean 4 已成为目前数学形式化工作的主流选择。\nAI4Math 是将人工智能技术应用于数学研究的跨学科领域。其目标是通过机器学习、大语言模型等方法，简化形式化过程中的繁重代码编写工作，辅助数学家进行数学形式化、定理证明乃至提出猜想。AI4Math 或将在将来大幅提高数学形式化和数学研究的效率。",
    "crumbs": [
      "形式化数学与 Lean 4 定理证明入门"
    ]
  },
  {
    "objectID": "00-poster.html#背景简介",
    "href": "00-poster.html#背景简介",
    "title": "形式化数学与 Lean 4 定理证明入门",
    "section": "",
    "text": "形式化数学是将数学定义、定理和证明转化为计算机可验证的精确形式的过程。可以认为，数学形式化 = 编写程序，而正确的证明 = 代码通过编译。严谨性是数学研究的基石，形式化数学通过严格的逻辑框架和程序语言确保数学结论的正确性和可复用性，在现代数学体系日渐庞大复杂的背景下具有重要意义。\nLean 4 是一个专为形式化数学设计的编程语言和证明助手，支持数学家将数学定理转化为计算机可验证的代码。它具备高效的编译器和灵活的类型系统，适合构建复杂的数学证明；其数学库 Mathlib 正在飞速扩展，目前已覆盖本科数学大部分内容。Lean 4 已成为目前数学形式化工作的主流选择。\nAI4Math 是将人工智能技术应用于数学研究的跨学科领域。其目标是通过机器学习、大语言模型等方法，简化形式化过程中的繁重代码编写工作，辅助数学家进行数学形式化、定理证明乃至提出猜想。AI4Math 或将在将来大幅提高数学形式化和数学研究的效率。",
    "crumbs": [
      "形式化数学与 Lean 4 定理证明入门"
    ]
  },
  {
    "objectID": "00-poster.html#课程信息",
    "href": "00-poster.html#课程信息",
    "title": "形式化数学与 Lean 4 定理证明入门",
    "section": "课程信息",
    "text": "课程信息\n本兴趣课程旨在向数学或计算机相关专业学生普及形式化数学的基本概念和方法，掌握 Lean 4 定理证明的基本技能，并了解 AI4Math 的最新进展，为后续更深入的探索做好引导。课程设计为 2025 秋季学期范洋宇老师《抽象代数》的配套课余活动，欢迎感兴趣的同学参与。\n本课程原则上不要求任何编程或数学背景，但建议具备至少一侧的常识。我们建议有意动手实操的同学携带电脑，并提前配置好网络、Git、VSCode 等相关环境。\n\n\n\n\n\n\n\n\n\n图 1: 课程群\n\n\n\n\n\n时间：1–12 周周三 18:30–20:05，首次课程 9 月 17 日\n地点：文萃楼 F502\n主讲人：钟星宇\n北京理工大学 2022 级强基数学专业本科生\nMathlib 4 Contributor\nICPC Regional Silver Medalist\n2025 BICMR–RUC 代数与形式化暑期学校学员\nBICMR–Ubiquant AI4Math 数据标注团队实习经验",
    "crumbs": [
      "形式化数学与 Lean 4 定理证明入门"
    ]
  },
  {
    "objectID": "00-poster.html#课程大纲供参考",
    "href": "00-poster.html#课程大纲供参考",
    "title": "形式化数学与 Lean 4 定理证明入门",
    "section": "课程大纲（供参考）",
    "text": "课程大纲（供参考）\n\nIntroduction to Formal Mathematics with Lean 4\nHello World\nLogic\nNumbers, Functions and Sets\nDependent Type Theory in Lean 4\nMathematical Analysis: Taking Limits on the Real Numbers\nRandom Lean 4 Tips\nAbstract Algebra: Group, Rings and Fields\nQuotient Types and Universal Properties\nInductive Types and Induction Methods\nClasses and Instances\nCoercions",
    "crumbs": [
      "形式化数学与 Lean 4 定理证明入门"
    ]
  },
  {
    "objectID": "00-poster.html#参考材料",
    "href": "00-poster.html#参考材料",
    "title": "形式化数学与 Lean 4 定理证明入门",
    "section": "参考材料",
    "text": "参考材料\n\nIntroductory:\n\nCAV2024\nTerence Tao at IMO 2024: AI and Mathematics\nLean 的前世今生\nNatural Number Game\nComputational Trilogy - nLab\n\nBibles\n\nMathematics in Lean 4\nTheorem Proving in Lean 4\nLean Language Manual\nType Theory - nLab\nOther bibles\n\nCourses\n\nKevin Buzzard’s 2024 course on formalising mathematics in the Lean theorem prover",
    "crumbs": [
      "形式化数学与 Lean 4 定理证明入门"
    ]
  },
  {
    "objectID": "00-poster.html#链接",
    "href": "00-poster.html#链接",
    "title": "形式化数学与 Lean 4 定理证明入门",
    "section": "链接",
    "text": "链接\n\nCourse materials:\n\ncourse repository\nonline documentation\n\nOnline compiler:\n\nLean 4 Web\n\nCommunity\n\nLean Zulip\n\nMiscellaneous\n\nLean 4 tactics cheatsheet",
    "crumbs": [
      "形式化数学与 Lean 4 定理证明入门"
    ]
  },
  {
    "objectID": "01-intro-code.html",
    "href": "01-intro-code.html",
    "title": "01-intro.lean",
    "section": "",
    "text": "import Mathlib\nFermat’s Last Theorem: if n &gt; 2, then there are no positive integer solutions to a^n + b^n = c^n.\ntheorem FLT (n : ℕ) (hn : n &gt; 2) (a b c : ℕ) : a ≠ 0 → b ≠ 0 → c ≠ 0 → a^n + b^n ≠ c^n := by\n  sorry\n\ndef TendsTo (a : ℕ → ℝ) (t : ℝ) : Prop :=\n  ∀ ε &gt; 0, ∃ n₀ : ℕ, ∀ n, n₀ ≤ n → |a n - t| &lt; ε\n\nexample : TendsTo (fun _ ↦ 998244353) 998244353 := by\n  unfold TendsTo\n  intro ε hε\n  use 19260817\n  intro n hn\n  simp [hε]\n\ntheorem tendsTo_add {a b : ℕ → ℝ} {A : ℝ} {B : ℝ} (ha : TendsTo a A) (hb : TendsTo b B) :\n    TendsTo (fun n =&gt; a n + b n) (A + B) := by\n  sorry\n\ntheorem tendsTo_sandwich {a b c : ℕ → ℝ} {L : ℝ} (ha : TendsTo a L) (hc : TendsTo c L)\n    (hab : ∀ n, a n ≤ b n) (hbc : ∀ n, b n ≤ c n) : TendsTo b L := by\n  sorry",
    "crumbs": [
      "01-intro.lean"
    ]
  },
  {
    "objectID": "04-logic.html",
    "href": "04-logic.html",
    "title": "04-logic.lean",
    "section": "",
    "text": "import Mathlib",
    "crumbs": [
      "04-logic.lean"
    ]
  },
  {
    "objectID": "04-logic.html#true-false-and-not",
    "href": "04-logic.html#true-false-and-not",
    "title": "04-logic.lean",
    "section": "1 True, False and Not",
    "text": "1 True, False and Not\nIn Lean’s dependent type theory, True and False are propositions serving as the terminal and initial objects in the universe of Prop.\nEagle-eyed readers may notice that True and False act similarly to singleton sets and empty sets in set theory.\nThey are constructed as inductive types.\nsection\n\nvariable (p q : Prop)\n\n\n1.1 True (⊤)\nTrue has a single constructor True.intro, which produces the unique proof of True. True is self-evidently true by True.intro.\n#check True.intro\nTrue as the terminal object\nexample : p → True := by\n  intro _\n  exact True.intro\nThe following examples shows that True → p is logically equivalent to p.\nexample (hp : p) : True → p := by\n  intro _\n  exact hp\n[IGNORE] Above is actually the elimination law of True.\nexample (hp : p) : True → p := True.rec hp\n\nexample (htp : True → p) : p := htp True.intro\ntrivial is a tactic that solves goals of type True using True.intro, though it’s power does not stop here.\nexample (htp : True → p) : p := by\n  apply htp\n  trivial\n\n\n1.2 False (⊥)\nFalse has no constructors, meaning that there is no way to construct a proof of False. This means that False is always false.\nFalse.elim is the eliminator of False, serve as the “principle of explosion”, which allows us to derive anything from a falsehood. False.elim is self-evidently true in Lean’s dependent type theory.\n#check False.elim\n#check False.rec -- [IGNORE] `False.elim` is actually defined as `False.rec`\neliminating False\nexample (hf : False) : p := False.elim hf\nexfalso is a tactic that applys False.elim to the current goal, changing it to False.\nexample (hf : False) : p := by\n  exfalso\n  exact hf\ncontradiction is a tactic that proves the current goal by finding a trivial contradiction in the context.\nexample (hf : False) : p := by\n  contradiction\n\n-- [EXR]\nexample (h : 1 + 1 = 3) : RiemannHypothesis := by\n  contradiction\nOn how to actually obtain a proof of False from a trivially false hypothesis via term-style proof [TODO], see here\n[IGNORE] Experienced audiences may question why False.elim lands in Sort* universe instead of Prop. This is because False is a subsingleton. See the manual to understand how the universe of a recursor is determined.\nend",
    "crumbs": [
      "04-logic.lean"
    ]
  },
  {
    "objectID": "04-logic.html#not",
    "href": "04-logic.html#not",
    "title": "04-logic.lean",
    "section": "2 Not (¬)",
    "text": "2 Not (¬)\nIn Lean’s dependent type theory, negation ¬p is realized as p → False\nYou may understand ¬p as “if p then absurd”, indicating that p cannot be true.\nsection\n\nvariable (p q : Prop)\n\n#print Not\nthis has a name absurd in Lean\n#check absurd\nexample (hp : p) (hnp : ¬p) : False := hnp hp\n[EXR] contraposition\nexample : (p → q) → (¬q → ¬p) := by\n  intro hpq hnq hp\n  exact hnq (hpq hp)\ncontrapose! is a tactic that does exactly this. We shall discuss this later.\n[EXR]\nexample : ¬True → False := by\n  intro h\n  exact h True.intro\n[EXR]\nexample : ¬False := by\n  intro h\n  exact h\n[EXR] double negation introduction\nexample : p → ¬¬p := by\n  intro hp hnp\n  exact hnp hp\nDouble negation elimination is not valid in intuitionistic logic. You’ll need proof by contradiction Classical.byContradiction to prove it. The tactic by_contra is created for this purpose. If the goal is p, then by_contra hnp changes the goal to False, and adds the hypothesis hnp : ¬p into the context.\n#check Classical.byContradiction\ndouble negation elimination\nexample : ¬¬p → p := by\n  intro hnnp\n  by_contra hnp\n  exact hnnp hnp\nYou can use the following command to check what axioms are used in the proof\n#print axioms Classical.not_not -- above has a name\nFor logical lunatics:\nIn Lean, Classical.byContradiction is proved by the fact that all propositions are Decidable in classical logic, which is a result of - the axiom of choice Classical.choice - the law of excluded middle Classical.em, which is a result of - the axiom of choice Classical.choice - function extensionality funext, which is a result of - the quotient axiom Quot.sound - propositional extensionality propext\nYou can always trace back like this in Lean, by ctrl-clicking the names. This is a reason why Lean is awesome for learning logic and mathematics.\n[EXR] another side of contraposition\nexample : (¬q → ¬p) → (p → q) := by\n  intro hnqnp hp\n  by_contra hnq\n  exact hnqnp hnq hp\n\nend\n[IGNORE] In fact above is equivalent to double negation elimination. This one use the have tactic, which allows us to state and prove a lemma in the middle of a proof.\nexample (hctp : (p q : Prop) → (¬q → ¬p) → (p → q)) : (p : Prop) → (¬¬p → p) := by\n  intro p hnnp\n  have h : (¬p → ¬True) := by\n    intro hnp _\n    exact hnnp hnp\n  apply hctp True p h\n  trivial",
    "crumbs": [
      "04-logic.lean"
    ]
  },
  {
    "objectID": "04-logic.html#pushing-negations",
    "href": "04-logic.html#pushing-negations",
    "title": "04-logic.lean",
    "section": "3 Pushing negations",
    "text": "3 Pushing negations\nSome negation can be pushed within intuitionistic logic. Some cannot.\n\n3.1 Negation with ∧ and ∨\nsection\n\nvariable (p q r : Prop)\nClassical logic: case analysis\nexample (hpq : p → q) (hnpq : ¬p → q) : q := Or.elim (Classical.em p) hpq hnpq\n#check Classical.byCases -- above has a name\nWe have a corresponding tactic: by_cases\nexample (hpq : p → q) (hnpq : ¬p → q) : q := by\n  by_cases hp : p\n  · exact hpq hp\n  · exact hnpq hp\nProof by cases would help us to obtain an equivalent characterization of Or.\nexample : (p ∨ q) ↔ (¬p → q) := by\n  constructor\n  · rintro (hp | hq)\n    · intro hnp\n      exfalso\n      exact hnp hp\n    · intro _\n      exact hq\n  · intro hnpq  -- the direction of constructing `Or` needs classical logic\n    by_cases h?p : p\n    · left; exact h?p\n    · right; exact hnpq h?p\nNote that this vividly illustrates the difference between classical logic and intuitionistic logic.\nIn intuitionistic logic, Or means slightly stronger than in classical logic: by p ∨ q we mean that we know explicitly which one of p and q is true. We cannot do implications like ¬p → q implying p ∨ q, because we don’t know exactly which one of p and ¬p is true, and the introduction rules of Or are asking us to provide it explicitly. This is a reason why intuitionistic logic is considered to be computable.\nWe also have an equivalent characterization of And. This is also done in classical logic.\nexample : (p ∧ q) ↔ ¬(p → ¬q) := by\n  constructor\n  · intro ⟨hp, hnq⟩ hpnq\n    exact hpnq hp hnq\n  · intro hnpnq -- the direction of constructing `And` needs classical logic\n    contrapose hnpnq\n    rw [Classical.not_not]\n    intro hp hq\n    exact hnpnq ⟨hp, hq⟩\n[EXR] →–∨ distribution\nexample : (r → p ∨ q) ↔ ((r → p) ∨ (r → q)) := by\n  constructor\n  · intro hrpq -- this direction needs classical logic\n    by_cases h?r : r\n    · rcases hrpq h?r with (hp | hq)\n      · left; intro _; exact hp\n      · right; intro _; exact hq\n    · left\n      intro hr\n      exfalso; exact h?r hr\n  · rintro (hrp | hrq)\n    · intro hr\n      left; exact hrp hr\n    · intro hr\n      right; exact hrq hr\n#check imp_or -- above has a name\n[EXR] De Morgan’s laws\nexample : ¬(p ∨ q) ↔ ¬p ∧ ¬q := by\n  constructor\n  · intro hnq\n    constructor\n    · intro hp\n      apply hnq\n      left; exact hp\n    · intro hq\n      apply hnq\n      right\n      exact hq\n  · rintro ⟨hnp, hnq⟩ (hp | hq)\n    · exact hnp hp\n    · exact hnq hq\n#check not_or -- above has a name\n[EXR] De Morgan’s laws\nexample : ¬(p ∧ q) ↔ ¬p ∨ ¬q := by\n  constructor\n  · intro hnpq -- this direction needs classical logic\n    by_cases h?p : p\n    · right\n      intro hq\n      apply hnpq\n      exact ⟨h?p, hq⟩\n    · left\n      exact h?p\n  · rintro (hnp | hnq) ⟨hp, hq⟩\n    · exact hnp hp\n    · exact hnq hq\n#check not_and -- above has a name\nIntroducing push_neg tactic: automatically proves all the above. It works in classical logic where negation normal forms exist.\nby_contra!, contrapose! are push_neg-enhanced version of their non-! counterparts.\nFor more exercises, see Propositions and Proofs - TPiL4\nend",
    "crumbs": [
      "04-logic.lean"
    ]
  },
  {
    "objectID": "04-logic.html#ignore-decidable",
    "href": "04-logic.html#ignore-decidable",
    "title": "04-logic.lean",
    "section": "4 [IGNORE] Decidable",
    "text": "4 [IGNORE] Decidable\nIt’s high time to introduce Decidable here for the first time.\nMathematicians are often aware of intuitionistic logic. They know classical logic is equipped with Classical.em: p ∨ ¬p for any proposition p. Though rarely do they know the concept of Decidable, which more often appears in the theory of computation.\nFor short, Decidable p means exactly the same as p ∨ ¬p in intuitionistic logic. It means that we know explicitly (or computationally) which one of p and ¬p is true.\nThough formally in Lean, Decidable is defined as a distinct inductive type, it is very similar to Or in that you may, somehow, even use it like a p ∨ ¬p. But there are major differences. They are:\n\n[IGNORE] Decidable lives in Type universe, instead of Prop universe.\nIn Lean’s dependent type theory, things in Prop universe are allowed to be non-constructive. This is because in Prop universe, proofs are proof-irrelevant: Lean forgets the exact proof of a proposition once it is proved. So when we have an Or, we actually have no idea which one of the two sides is true. Lean is designed so, probably because most of the mathematics is non-constructive.\nOn the other hand, things in Type universe are required to be constructive, unless you have used Classical.choice (In such situation, Lean will require you to tag it as noncomputable).\nDecidable is designed to be constructive, because it is used to decide whether a proposition is true or false by computation. So Decidable must live in Type universe: To save whether p or ¬p is true.\nIn short, Prop is non-constructive and proof-irrelevant, while Type is constructive and saves data. This makes Decidable stronger than a pure proof of p ∨ ¬p : Prop.\n[IGNORE] It is tagged as a typeclass.\nThis allows Lean to automatically find a proof of Decidable p so that you don’t have to prove it yourself.\nSo at many places Decidable p is implicitly deduced.\nThe constructors of Decidable has different names: isTrue and isFalse\n\nTo wrap up, we have Decidable because:\n\nTo mean exactly the same as p ∨ ¬p in intuitionistic logic, to make it computable.\nTo allow you to just assume p ∨ ¬p for only some propositions, which is more flexible than a classical logic overkill.\n\nsection\n\nvariable (p q : Prop)\n\n#print Decidable\n#check Decidable.isTrue\n#check Decidable.isFalse\nDecidable enables computational reasoning to see if a proposition is true or false\n#eval True\n#eval True → False\n#eval False → (1 + 1 = 3)\n#synth Decidable (False → (1 + 1 = 3))\nManually proving Decidable to ensures a computable proof\ninstance : Decidable (p → p ∨ q) := by\n  apply Decidable.isTrue -- explicit use of constructor\n  intro hp\n  left\n  exact hp\n#synth Decidable (p → p ∨ q)\n#eval (p q : Prop) → (p → (p ∨ q))\nDecidable enables partial classical logic\n#check Classical.byContradiction -- we have done this before\nproof by contradiction in intuitionistic logic with decidable hypothesis\nexample [dp : Decidable p] : (¬p → False) → p := by\n  intro hnpn\n  rcases dp with (hnp | hp)\n  · exfalso; exact hnpn hnp\n  · exact hp\n#check Decidable.byContradiction -- above has a name\n\nend",
    "crumbs": [
      "04-logic.lean"
    ]
  },
  {
    "objectID": "05-type.html",
    "href": "05-type.html",
    "title": "05-type.lean",
    "section": "",
    "text": "import Mathlib",
    "crumbs": [
      "05-type.lean"
    ]
  },
  {
    "objectID": "05-type.html#numbers",
    "href": "05-type.html#numbers",
    "title": "05-type.lean",
    "section": "1 Numbers",
    "text": "1 Numbers\nLean and Mathlib have many built-in types for numbers, including\n#check ℕ\n#check ℤ\n#check ℚ\n#check ℝ\n#check ℂ\nThere are some built-in ways to represent numbers. Lean interprets their types accordingly, like every programming language does.\n#check 3\n#check 3.14\n#check (22 / 7 : ℚ)\n#check Real.pi\n#check Complex.log (-1) / Complex.I\nDo note that numbers in different types work differently. Sometimes you need to explicitly specify the type you want.\n#eval 22 / 7\n#eval (22 : ℚ) / 7\n#eval (22 / 7 : ℚ)\nYou may not #eval (22 : ℝ) / 7 because ℝ is not computable. It’s defined using Cauchy sequences of rational numbers. For Float computation you may use Float type.\n#eval (22 : Float) / 7\nStrange as it may seem, this type checks.\n#check (Real.sqrt 2) ^ 2 = (5 / 2 : ℕ)\nNote how the type of a number is interpreted and implicitly coerced.\nCoercions are automatic conversions between types. It somewhat allows us to abuse notations like mathematicians always do. Detailing coercions would be another ocean of knowledge. We shall stop here for now.",
    "crumbs": [
      "05-type.lean"
    ]
  },
  {
    "objectID": "05-type.html#universe-hierarchy",
    "href": "05-type.html#universe-hierarchy",
    "title": "05-type.lean",
    "section": "2 Universe hierarchy",
    "text": "2 Universe hierarchy\nIf everything has a type, what is the type of a type?\n#check 3\n#check Nat\n#check Type\n#check Type 1\n#check Type 2\n\n#check 1 + 2 = 3\n#check Prop\nLean has a hierarchy of universes:\n  ...\n   ↓\nType 3            = Sort 4\n   ↓\nType 2            = Sort 3\n   ↓\nType 1            = Sort 2\n   ↓\n Type   = Type 0  = Sort 1\n   ↓\n Prop   =  Sort   = Sort 0\n\nProp is the universe of logical propositions.\nType is the universe of most of the mathematical objects.\n\nAt most times, you don’t need to care about universe levels above Type. But do recall the critical difference between Prop and Type:\nTerms in Prop, i.e. proofs, are proof-irrelevant, i.e. all proofs of the same proposition are considered equal, while terms in Type are distinguishable in general. This allows classical reasoning in Prop, and computation in Type.\nYou may explore more on this in the previous logic chapters.\n\n2.1 Remark\nTwo questions arise naturally here:\n\nWhy Prop is separated from Type?\nThis is answered by the need of proof irrelevance.\nWhy Prop is at the bottom of the hierarchy?\nWe come up with two explanations ([TODO] discussions are welcome!):\n\nProp is often compared to Bool : Type. This analogy validates the Prop : Type convention.\nBool has two values true and false, representing truth values, acting as a switch. Prop may be viewed as a non-computatble version of Bool, switching by whether a proposition is true or false. e.g. In Mathlib, a subset of α is defined as a predicate α → Prop, a relation on α is defined as α → α → Prop, etc. But all of these are non-computable. e.g. you cannot define a computable function by this switch.\nOn determining universe levels of predicates and functions.\nFor a function α → β where α : Type u and β : Type v, its should live in Type (max u v) naturally. But recall ∀ and ∃ quantifiers from logic. They eat α → Prop functions to produce propositions, living in Prop. This means that Prop should be larger than any Type u to accommodate such functions. As a convention, we put Prop at the bottom of the hierarchy to reflect this. The true universe level of a function is imax u v if it maps from Sort u to Sort v, where imax is the regular max except that imax u 0 = imax 0 u = 0 for any u.",
    "crumbs": [
      "05-type.lean"
    ]
  },
  {
    "objectID": "05-type.html#equality-eq-first-visit",
    "href": "05-type.html#equality-eq-first-visit",
    "title": "05-type.lean",
    "section": "3 Equality Eq (=) (first visit)",
    "text": "3 Equality Eq (=) (first visit)\nEquality is a fundamental notation in mathematics, but also a major victim of abuse of notation. Though trained experts can usually tell from context what kind of equality is meant, things still become hopelessly confusing from time to time.\nIn set theory, by axiom of extensionality, two sets are equal if and only if they have the same elements.\nIn Lean’s type theory, we distinguish between different equalities:\n\nDefinitional equality\nPropositional equality (Eq, i.e. =)\nHeterogeneous equality (We shall not touch this)\n\nWe shall now show the basic usage of = in Lean, mostly in tactic mode. We detail a little on the difference between definitional and propositional equality afterwards. We postpone the real, full discussion of equality to later chapters.\nsection\nEq takes two terms of the same type (up to definitional equality), and produces a proposition in Prop. For terms a b : α, the proposition a = b means that a and b are equal. Do note that types of a and b must be the same, i.e. definitionally equal.\n#check Eq\n#check 1 + 1 = 3\n-- #check 1 + 1 = Nat -- this won't compile. Eq requires both sides to have the same type.\n\n3.1 Handling equality\nvariable (a b c : ℚ)\nThe most basic way to show an equality is by tactic rfl: LHS is definitionally equal to RHS.\nexample : a = a := rfl\nNote that rfl works for not only literally-the-same terms, but also definitionally equal terms. We’ll detail definitional equality afterwards.\nrw is a tactic that rewrites a goal by a given equality.\nexample (f : ℚ → ℚ) (hab : a = b) (hbc : b = c) : f a = f c := by\n  rw [hab, hbc]\nyou may also apply the equality in the reverse direction\nexample (f : ℚ → ℚ) (hab : b = a) (hbc : b = c) : f a = f c := by\n  rw [← hab, hbc]\nYou may also use symm tactic to swap an equality\n#help tactic symm\nexample (f : ℚ → ℚ) (hab : b = a) (hbc : b = c) : f a = f c := by\n  symm at hab\n  rw [hab, hbc]\nor swap at the goal\nexample (f : ℚ → ℚ) (hab : b = a) : f a = f b := by\n  symm\n  rw [hab]\nYou may also rewrite at a hypothesis.\nexample (hab : a = b) (hbc : b = c) : a = c := by\n  rw [hbc] at hab\n  exact hab\ncongr tactic reduces the goal f a = f b to a = b.\n#help tactic congr\nexample (f : ℚ → ℚ) (hab : a = b) (hbc : b = c) : f a = f c := by\n  congr\n  rw [hab, hbc]\n\n\n3.2 Working in CommRing\nLet’s do some basic rewrites in commutative rings, e.g. ℚ.\n\nCommutativity and associativity\n#check add_comm\nexample : a + b = b + a := by rw [add_comm]\n\n#check add_assoc\nexample : (a + b) + c = a + (b + c) := by rw [add_assoc]\n\n#check mul_comm\nexample : a * b = b * a := by rw [mul_comm]\n\n#check mul_assoc\nexample : (a * b) * c = a * (b * c) := by rw [mul_assoc]\nSometimes you need to specify the arguments to narrow down possible targets for rw.\nexample : (a + b) + c = (b + a) + c := by\n  rw [add_comm a b]\n[EXR] You may chain multiple rewrites in one rw.\nexample : (a + b) + c = a + (c + b) := by\n  rw [add_assoc, add_comm b c]\n\n-- [EXR]\nexample : a + b + c = c + a + b := by\n  rw [add_comm, add_assoc]\n\n#check mul_add\nexample : (a + b) * c = c * a + c * b := by\n  rw [mul_comm, mul_add]\n\n-- [EXR]\nexample : (a + b) * (c + b) = a * c + a * b + b * c + b * b := by\n  rw [add_mul, mul_add, mul_add, ← add_assoc]\n\n\nZero and one\n#check add_zero\nexample : a + 0 = a := by rw [add_zero]\n#check zero_add\nexample : 0 + a = a := by rw [zero_add]\n\n#check mul_one\nexample : a * 1 = a := by rw [mul_one]\n#check one_mul\nexample : 1 * a = a := by rw [one_mul]\n\n-- [EXR]\nexample : 1 * a + (0 + b) * 1 = a + b := by\n   rw [one_mul, zero_add, mul_one]\n[EXR] uniqueness of zero\nexample (o : ℚ) (h : ∀ x : ℚ, x + o = x) : o = 0 := by\n  specialize h 0\n  rw [zero_add] at h\n  exact h\n\n\nSubtraction\ntransposition\n#check add_sub_assoc\n#check sub_self\n#check add_zero\nexample (h : c = a + b) : c - b = a := by\n  rw [h, add_sub_assoc, sub_self, add_zero]\n\n\nAutomation\nHad enough of these tedious rewrites? Automation makes your life easier.\nsimp (at h) tactic eliminates 0 and 1 automatically. simp? shows you what lemmas simp used.\n#help tactic simp\nexample : c + a * (b + 0) = a * b + c := by\n  simp\n  rw [add_comm]\nring tactic is even stronger: it reduces LHS and RHS to a canonical form (it exists in any commutative ring) to solve equalities automatically. ring_nf (at h) reduces the expression h to its canonical form.\n#help tactic ring -- check out the documentation\nexample : (a + 1) * (b + 2) = a * b + 2 * a + b + 2 := by\n  ring\napply_fun at h tactic applies a function to both sides of an equality hypothesis h. Combined with simp and ring, it make transpotions easier.\n#help tactic apply_fun\nexample (h : a + c = b + c) : a = b := by\n  apply_fun (fun x ↦ x - c) at h\n  simp at h\n  exact h\n[EXR] transposition again\nexample (h : c = a + b) : c - b = a := by\n  apply_fun (fun x ↦ x - b) at h\n  simp at h\n  exact h\n\n\nA remark on type classes\nWondering how Lean knows that commutativity, associativity, distributivity, etc. hold for ℚ? Wondering how Lean knows a * 1 = a and has relevant lemmas for that? This is because Lean knows that ℚ is an commutative ring. This is because in Mathlib, ℚ has been registered as an instance of the typeclass CommRing. So that once you import Mathlib, Lean automatically knows about the CommRing structure of ℚ. We might learn about typeclasses later in this course.\n#synth CommRing ℚ -- Checkout the `CommRing` instance that Mathlib provides for `ℚ`\n\n\n\n3.3 funext and propext\nThere are several ways to show a (propositional) equality other than rfl and rw.\nFunctional extensionality funext states that two functions are equal if they give equal outputs for every input.\nIt’s a theorem in Lean’s type theory, derived from the quotient axiom Quot.sound.\n#check funext\nexample (f g : ℚ → ℚ) (h : ∀ x : ℚ, f x = g x) : f = g := funext h\nIt has a tactic version ext / funext as well\n#help tactic funext\nexample (f g : ℚ → ℚ) (h : ∀ x : ℚ, f x = g x) : f = g := by\n  funext x\n  exact h x\nPropositional extensionality propext states that two propositions are equal if they are logically equivalent. It’s admitted as an axiom in Lean.\n#check propext\nexample (P Q : Prop) (h : P ↔ Q) : P = Q := propext h\nIt has a tactic version ext as well\n#help tactic ext\nexample (P Q : Prop) (h : P ↔ Q) : P = Q := by\n  ext\n  exact h\nThis allows you to rw an iff (↔︎) like an equality (=).\nexample (P Q : Prop) (h : P ↔ Q) : P = Q := by\n  rw [h]",
    "crumbs": [
      "05-type.lean"
    ]
  },
  {
    "objectID": "05-type.html#defining-terms-and-functions",
    "href": "05-type.html#defining-terms-and-functions",
    "title": "05-type.lean",
    "section": "4 Defining terms and functions",
    "text": "4 Defining terms and functions\nWe now come back to detail a little on the exact power of rfl, i.e. what is the meaning of definitional equality.\nFirst, we show how to define terms and functions in Lean.\n\n4.1 Global definitions\nRecall that you may use def to define your own terms.\ndef myNumber : ℚ := 998244353\n#check myNumber\ndef can also define functions.\n#check fun (x : ℚ) ↦ x * x\ndef square (x : ℚ) : ℚ := x * x\ndef square' : ℚ → ℚ := fun x ↦ x * x\n#print square\n#print square'\nBe open minded: you may even use tactic mode to define terms!\ndef square'' : ℚ → ℚ := by\n  intro x\n  exact x * x\n#print square''\n\ndef square_myNumber : ℚ := by\n  apply square\n  exact myNumber\n#print square_myNumber\n\n\n4.2 Local definitions\nYou may also define local terms and functions using let. It may be used in both term mode and tactic mode.\n#help tactic let\n\nexample : ℚ := by\n  let a : ℚ := 3\n  let b : ℚ := 4\n  exact square (a + b)\n\nexample : ℚ :=\n  let a : ℚ := 3\n  let b : ℚ := 4\n  square (a + b)\n\nexample : let a := 4; let b := 4; a = b := rfl\nSometimes you want an alias for a complex term. set tactic is a variant of let that automatically replaces all occurrences of the defined term.\n#help tactic set\n\nexample (a b c : ℕ) : 0 = a + b - (a + b) := by\n  set d := a + b\n  simp\nIt’s crucial to distinguish between let and have: let saves the term of the definition for later use, but have is “opaque”: it won’t let you unfold the definition later. Thus naturally, let is often used for Type*s, and have is used for Props.\nexample : 3 = 3 := by\n  let a := 3\n  let b := 3\n  have h : a = b := rfl\n  exact h\n\nexample : 3 = 3 := by\n  have a := 3\n  have b := 3\n  -- have h : a = b := rfl\n  sorry -- above won't compile\n[TODO] Explain why it works here.\nexample : have a := 3; have b := 3; a = b := rfl\n\n\n4.3 Unfolding definitions\nTo manually unfold a definition in the tactic mode, you may use the rw (at h) tactic or the unfold (at h) tactic.\n#help tactic unfold\nexample : square myNumber = 998244353 * 998244353 := by\n  rw [square]\n  unfold myNumber\n  rfl\nFor local (non-have) definitions, you may use unfold as well. Though sadly rw does not work for local definitions for now.\nexample (a b : ℕ): (a + b) - (a + b) = 0 := by\n  set d := a + b\n  unfold d\n  simp\nLuckily, have, let and set all allows you to obtain a propositional equality when defining. (Technically this is not an unfolding, though.)\nexample (a b : ℕ) : (a + b) - (a + b) = 0 := by\n  let (eq := h1) d1 := a + b\n  have (eq := h2) d2 := a + b\n  set d3 := a + b with h3\n  simp\n\n\n4.4 Definitional equality vs propositional equality\n[IGNORE] Skip this if you find it confusing for the first time. You can recall this when we deal with quotient types.\nDefinitional equality means that two terms are the same by definition (i.e. they reduce to the same form).\n\ndef, theorem-like commands\nApplications of functions\n\nare examples of definitional equalities.\nIt is a meta-level concept, it cannot be stated as a proposition.\n\nrfl\nAs the sole constructor of propositional equality, rfl proves a definitional equality.\n#check rfl\nNote that myNumber is definitionally equal to 998244353.\nexample : myNumber = 998244353 := rfl\nrfl can even solve simple evaluations, because both sides reduce to 8 by the (inductive) definition of arithmetic operations over ℕ.\nexample : 5 + 3 = 2 * 2 * 2 := rfl\nrfl also has a tactic version. This tactic works for logical equivalences (↔︎) as well, as Iff.rfl does.\n#help tactic rfl\n\nexample : True ↔ True := by rfl\nThese are some non-examples for definitional equality. They are only propositionally equal, by propext and logical equivalence.\n-- example (p : Prop) : True ↔ (p → True) := by rfl\n-- example True ↔ ¬ False := by rfl\n\n\nType checking\nType checking is determined up to definitional equality.\nIn fact, it’s the sole responsibility of Lean’s compiler to check definitional equalities.\nAn failure of definitional equality results in a type error. That is, it is regarded as invalid Lean code.\ndef myType := ℚ\nThis won’t compile, because Lean do not know a coercion of ℕ → myType.\n-- def myTypeNumber := (998244353 : myType)\nThis passes the type check. because we manually build a bridge here: Lean knows the coercion ℕ → ℚ and that myType is definitionally equal to ℚ.\ndef myTypeNumber : myType := (998244353 : ℚ)\n#check myTypeNumber\nThis also passes the type check for the same reason.\n#check myTypeNumber = myNumber\nThe type of myNumber : ℚ and myTypeNumber : myType are definitionally equal, thus the equality passes the type check. Their values are also definitionally equal, so you can prove their equality by rfl.\nexample : myTypeNumber = myNumber := rfl\nabbrev defines an abbreviation, which is like a def, but always expands when processed. This is useful for type synonyms.\nabbrev myAbbrev := ℚ\ndef myAbbrevNumber : myAbbrev := 998244353\n#check myAbbrevNumber\n\n\nPropositional equality\nPropositional equality is\n\ndefined as the inductive type Eq (notation =),\nconstructed by the constructor rfl (reflexivity, i.e. a = a), with propext and Quot.sound as extra axioms (funext is an corollary of Quot.sound),\neliminated by the rw tactic (in practice).\n\nPropositional equality is not a meta-level concept. It’s a proposition in Prop that may be proved or disproved.\nPropositional equality on types does not get the types check. For example, this won’t compile.\n-- example (α : Type) (h : α = ℕ) (a : α) : a = (998244353 : ℕ) := by sorry\n\nend",
    "crumbs": [
      "05-type.lean"
    ]
  },
  {
    "objectID": "05-type.html#equality-second-visit",
    "href": "05-type.html#equality-second-visit",
    "title": "05-type.lean",
    "section": "5 Equality (second visit)",
    "text": "5 Equality (second visit)\n[TODO] (This section is for future chapters.)\n#print Eq\n#print Equivalence\n#print Setoid\n#print PartialOrder",
    "crumbs": [
      "05-type.lean"
    ]
  },
  {
    "objectID": "08-group.html",
    "href": "08-group.html",
    "title": "08-group.lean",
    "section": "",
    "text": "import Mathlib",
    "crumbs": [
      "08-group.lean"
    ]
  },
  {
    "objectID": "08-group.html#semigroups",
    "href": "08-group.html#semigroups",
    "title": "08-group.lean",
    "section": "1 Semigroups",
    "text": "1 Semigroups\n\n1.1 Objects\nA Semigroup is a type with an associative binary operation *.\nsection\n\n#check Semigroup\nvariable (G : Type*) [Semigroup G] (a b c : G)\nexample : a * (b * c) = (a * b) * c := by rw [mul_assoc]\nAn AddSemigroup is exactly the same as Semigroup, only with additive + notation.\nvariable (A : Type*) [AddSemigroup A] (a b c : A)\nexample : a + (b + c) = (a + b) + c := by rw [add_assoc]\nNote that using the notation of + does not necessarily mean that the operation is commutative. To this end, we have CommSemigroup and AddCommSemigroup.\n#check CommSemigroup\n#check mul_comm\n\n#check AddCommSemigroup\n#check add_comm\n\nend\n\n\n1.2 Morphisms\nA MulHom is a morphism between two semigroups that preserves the multiplication. The notation for this is G₁ →ₙ* G₂.\nIt’s a bundle of:\n\na function f : G₁ → G₂\na proof that f preserves multiplication.\n\nStrictly speaking, this definition does not require the * operation to be associative on G₁ or G₂.\nsection\n\n#check MulHom\nvariable {G₁ G₂ G₃ : Type*} [Semigroup G₁] [Semigroup G₂] [Semigroup G₃]\n         (f : G₁ →ₙ* G₂) (g : G₂ →ₙ* G₃) (a b : G₁)\nexample : f (a * b) = f a * f b := by rw [map_mul]\nAdditive version of MulHom is AddHom (→ₙ+).\n#check AddHom\nYou might already notice that a MulHom can be used just like a function. This is because Mathlib has instantiated the FunLike type class to MulHom, which provides the function coercion.\n#synth FunLike (MulHom G₁ G₂) G₁ G₂\nCreating a new MulHom requires providing all the data needed.\n#check MulHom.mk\nexample : ℕ →ₙ+ ℕ := ⟨(· * 2), by intros; ring⟩\nComposition of MulHoms as functions preserves multiplication.\nexample : G₁ →ₙ* G₃ := ⟨g ∘ f, by intros; dsimp; rw [map_mul, map_mul]⟩\nTo avoid manually constructing MulHom every time when composing, We may use MulHom.comp g f. The dot convention g.comp f is used here for convenience.\nexample : (g.comp f) (a * b) = (g.comp f) a * (g.comp f) b := by simp\nA MulHom is determined by the underlying function.\n#check MulHom.ext\nexample (f₁ : G₁ →ₙ* G₂) (f₂ : G₁ →ₙ* G₂) (h : f₁.toFun = f₂.toFun) : f₁ = f₂ := by\n  ext x\n  change f₁.toFun x = f₂.toFun x\n  rw [h]\nAbove shows the bundled definition of MulHom, how to create it, and how to compose them. The same philosophy is adopted for other morphism-like structures in Mathlib, such as MonoidHom.\nend",
    "crumbs": [
      "08-group.lean"
    ]
  },
  {
    "objectID": "08-group.html#monoids",
    "href": "08-group.html#monoids",
    "title": "08-group.lean",
    "section": "2 Monoids",
    "text": "2 Monoids\n\n2.1 Objects\nA Monoid is a Semigroup with an identity element 1 s.t. a * 1 = a and 1 * a = a.\nsection\n\n#check Monoid\nvariable (G : Type*) [Monoid G] (a b c : G)\nexample : a * 1 = a := by rw [mul_one]\nexample : 1 * a = a := by rw [one_mul]\n[EXR] characterization of the identity element\nexample (h : ∀ x : G, x * a = x) : a = 1 := by\n  specialize h 1\n  rw [one_mul] at h\n  exact h\nMonoid additionaly enables power notation a ^ n for natural number n.\n#check Monoid.npow\nexample : a ^ 0 = 1 := by rw [pow_zero]\nexample (n : ℕ) : a ^ (n + 1) = a ^ n * a := by rw [pow_succ]\nWe are not prepared to prove this until we talk about induction.\n#check one_pow\nAddMonoid is the additive version of Monoid.\nvariable (A : Type*) [AddMonoid A] (a b c : A)\nexample : a + 0 = a := by rw [add_zero]\nexample : 0 + a = a := by rw [zero_add]\nexample : 0 • a = 0 := by rw [zero_smul]\nexample (n : ℕ) : (n + 1) • a = n • a + a := by rw [succ_nsmul]\nFor commutative monoids, we have CommMonoid and AddCommMonoid.\n#check CommMonoid\n#check AddCommMonoid\n\nend\n\n\n2.2 Morphisms\nA MonoidHom is a morphism between two monoids that preserves the multiplication and the identity. The notation for this is G →* H.\nsection\n\n#check MonoidHom\nvariable {G₁ G₂ G₃ : Type*} [Monoid G₁] [Monoid G₂] [Monoid G₃]\n         (f : G₁ →* G₂) (g : G₂ →* G₃) (a b : G₁)\nexample : f (a * b) = f a * f b := by rw [map_mul]\nexample : f 1 = 1 := by rw [map_one]\nAdditive version of MonoidHom is AddMonoidHom (→+).\n#check AddMonoidHom\nMonoidHom need additional data to MulHom: preservation of 1.\n#check MonoidHom.mk\nexample : ℕ →+ ℕ := ⟨⟨(· * 2), by simp⟩, by intros; ring⟩\n\nend",
    "crumbs": [
      "08-group.lean"
    ]
  },
  {
    "objectID": "08-group.html#groups",
    "href": "08-group.html#groups",
    "title": "08-group.lean",
    "section": "3 Groups",
    "text": "3 Groups\n\n3.1 Objects\nIn Mathlib, a Group is defined to be a Monoid where every element a has an left inverse a⁻¹ s.t. a⁻¹ * a = 1.\nsection\n\n#check Group\nvariable (G : Type*) [Group G] (a b c : G)\n#check a⁻¹\nexample : a⁻¹ * a = 1 := by rw [inv_mul_cancel]\nThe following exercises lead to a proof of: In a group, a left inverse is also a right inverse. This recovers the usual definition of a group.\n[EXR] left multiplication is injective\nexample (h : a * b = a * c) : b = c := by\n  apply_fun (a⁻¹ * ·) at h\n  rw [← mul_assoc, ← mul_assoc, inv_mul_cancel, one_mul, one_mul] at h\n  exact h\n#check mul_left_cancel -- this has a name\n[EXR] a left inverse actually also a right inverse\nexample : a * a⁻¹ = 1 := by\n  apply_fun (a⁻¹ * ·)\n  · dsimp\n    rw [← mul_assoc, inv_mul_cancel, one_mul, mul_one]\n  · apply mul_left_cancel\n#check mul_inv_cancel -- this has a name\nThe following proves that G is a DivisionMonoid. You don’t need to know what this means for now.\n#synth DivisionMonoid G\n[EXR] characterization of a right inverse\nexample (h : a * b = 1) : b = a⁻¹ := by\n  -- if you does not want to use `apply_fun`\n  rw [← one_mul b, ← inv_mul_cancel a, mul_assoc, h, mul_one]\n#check eq_inv_of_mul_eq_one_right -- this has a name\n[EXR] characterization of a left inverse\nexample (h : a * b = 1) : a = b⁻¹ := by\n  apply_fun (· * b⁻¹) at h\n  rw [mul_assoc, mul_inv_cancel, mul_one, one_mul] at h\n  exact h\n#check eq_inv_of_mul_eq_one_left -- this has a name\n[EXR] involutivity of the inverse\nexample : (a⁻¹)⁻¹ = a := by\n  symm; apply eq_inv_of_mul_eq_one_right\n  exact inv_mul_cancel a\n#check inv_inv -- this has a name\n[EXR] inverse of a product\nexample : (a * b)⁻¹ = b⁻¹ * a⁻¹ := by\n  apply inv_eq_of_mul_eq_one_left\n  rw [← mul_assoc, mul_assoc b⁻¹, inv_mul_cancel, mul_one, inv_mul_cancel]\n#check mul_inv_rev -- this has a name\nsome other injectivity\n[EXR] inverse is injective\nexample (h : a⁻¹ = b⁻¹) : a = b := by\n  apply_fun (·⁻¹) at h\n  rw [inv_inv a, inv_inv b] at h\n  exact h\n#check inv_injective -- this has a name\n[EXR] right multiplication is injective\nexample (h : b * a = c * a) : b = c := by\n  apply_fun (· * a⁻¹) at h\n  rw [mul_assoc, mul_assoc, mul_inv_cancel, mul_one, mul_one] at h\n  exact h\n#check mul_right_cancel -- this has a name\nwheelchair tactic for groups\n#help tactic group\nexample : (a ^ 3 * b⁻¹)⁻¹ = b * a⁻¹ * (a ^ 2)⁻¹  := by\n  group\n[TODO] DivInvMonoid enables zpow notation for integer powers a ^ n where n : ℤ.\n[IGNORE] It extends npow for monoids. See the library note [forgetful inheritance] for the philosophy of this definition.\nAdditive and commutative versions of groups are as usual.\n#check AddGroup\n#check CommGroup\n#check AddCommGroup\n\nend\n\n\n3.2 Morphisms\nMonoidHom It is also used for group homomorphisms.\nsection\n\nvariable {G₁ G₂ G₃ : Type*} [Group G₁] [Group G₂] [Group G₃]\n         (f : G₁ →* G₂) (g : G₂ →* G₃) (a b : G₁)\n[EXR] Monoid homomorphisms preserve inverses\nexample : f (a⁻¹) = (f a)⁻¹ := by\n  apply eq_inv_of_mul_eq_one_right\n  rw [← map_mul, mul_inv_cancel, map_one]\n#check map_inv -- this has a name\n[EXR] MonoidHom requires one to show preservation of 1. But this is redundant for group homomorphisms.\nexample (φ : G₁ →ₙ* G₂) : φ 1 = 1 := by\n  haveI : φ 1 * φ 1 = φ 1 * 1 := by rw [← map_mul, mul_one, mul_one]\n  exact mul_left_cancel this\nHence in the case of groups, Mathlib provides a constructor MonoidHom.mk' that only requires the preservation of multiplication to build a MonoidHom.\n#check MonoidHom.mk'\n\nend",
    "crumbs": [
      "08-group.lean"
    ]
  },
  {
    "objectID": "07-analysis.html",
    "href": "07-analysis.html",
    "title": "07-analysis.lean",
    "section": "",
    "text": "Mathematical Analysis\nFinally some real math in Lean! In this file we show how to define limits of sequences and continuity of functions in Lean. Of course it is just a toy version, far from the real Mathlib definitions. Nevertheless, that should be enough for you to get a taste of formalizing something that is not completely trivial.\nSince we haven’t touch division quite much yet, you may find it’s difficult to deal with multiplication and division. field_simp tactic may help you a lot in such cases. It won’t break things up as simp does. Anyway, don’t worry too much about it for now.\nimport Mathlib\n\ndef TendsTo (a : ℕ → ℝ) (t : ℝ) : Prop :=\n  ∀ ε &gt; 0, ∃ n₀ : ℕ, ∀ n, n₀ ≤ n → |a n - t| &lt; ε\n[EXR] The limit of the constant sequence with value c is c.\ntheorem tendsTo_const (c : ℝ) : TendsTo (fun _ ↦ c) c := by\n  unfold TendsTo\n  intro ε hε\n  use 1\n  intro n hn\n  simp [hε]\n- commutes with tendsTo\ntheorem tendsTo_neg {a : ℕ → ℝ} {t : ℝ} (ha : TendsTo a t) : TendsTo (fun n ↦ -a n) (-t) := by\n  unfold TendsTo\n  intro ε hε\n  specialize ha ε hε\n  rcases ha with ⟨n₀, hn₀⟩\n  use n₀\n  intro n hn\n  specialize hn₀ n hn\n  simp\n  -- what theorems should I use?\n  rw [← abs_neg, add_comm]\n  simp\n  -- what theorems should I use?\n  rw [← sub_eq_add_neg]\n  exact hn₀\n+ commutes with tendsTo\ntheorem tendsTo_add {a b : ℕ → ℝ} {A : ℝ} {B : ℝ} (ha : TendsTo a A) (hb : TendsTo b B) :\n    TendsTo (fun n =&gt; a n + b n) (A + B) := by\n  intro ε hε\n  specialize ha (ε / 2) (by linarith only [hε])\n  specialize hb (ε / 2) (by linarith only [hε])\n  rcases ha with ⟨n₀, ha⟩\n  rcases hb with ⟨m₀, hb⟩\n  use max n₀ m₀\n  intro n hn\n  -- what theorems should I use?\n  rw [max_le_iff] at hn\n  specialize ha n (by linarith only [hn.left])\n  specialize hb n (by linarith only [hn.right])\n  simp\n  -- common tactic: eliminate abs to make use of `linarith`\n  -- what theorems should I use?\n  rw [abs_lt] at ha hb ⊢\n  constructor\n  · linarith only [ha, hb]\n  · linarith only [ha, hb]\n[EXR] - commutes with tendsTo\ntheorem tendsTo_sub {a b : ℕ → ℝ} {A B : ℝ} (ha : TendsTo a A) (hb : TendsTo b B) :\n    TendsTo (fun n =&gt; a n - b n) (A - B) := by\n  haveI := tendsTo_add ha (tendsTo_neg hb)\n  -- [TODO] `congr` closes the goal directly here. Find out why.\n  ring_nf at this\n  exact this\n≤ version of TendsTo is equivalent to the usual TendsTo.\ndef TendsTo_le (a : ℕ → ℝ) (t : ℝ) : Prop :=\n  ∀ ε &gt; 0, ∃ n₀ : ℕ, ∀ n, n₀ ≤ n → |a n - t| ≤ ε\n\n-- [EXR]\ntheorem tendsTo_le_iff_TendsTo {a : ℕ → ℝ} {t : ℝ} : TendsTo_le a t ↔ TendsTo a t := by\n  constructor\n  · intro h ε hε\n    rcases h (ε / 2) (by linarith [hε]) with ⟨n₀, hn₀⟩\n    use n₀\n    intro n hn; specialize hn₀ n hn\n    linarith only [hn₀, hε]\n  · intro h ε hε\n    rcases h ε hε with ⟨n₀, hn₀⟩\n    use n₀\n    intro n hn; specialize hn₀ n hn\n    linarith only [hn₀, hε]\na weaker version of TendsTo where we require ε &lt; l. When l &gt; 0, this is equivalent to TendsTo.\ndef TendsTo_εlt (a : ℕ → ℝ) (t : ℝ) (l : ℝ) : Prop :=\n  ∀ ε &gt; 0, ε &lt; l → ∃ n₀ : ℕ, ∀ n, n₀ ≤ n → |a n - t| &lt; ε\n\ntheorem tendsTo_εlt_iff_TendsTo {a : ℕ → ℝ} {t : ℝ} {l : ℝ} (l_gt_zero : l &gt; 0) :\n    TendsTo_εlt a t l ↔ TendsTo a t := by\n  constructor\n  · intro h ε hε\n    specialize h (min ε (l / 2))\n                 (by apply lt_min; all_goals linarith)\n                 (by apply min_lt_of_right_lt; linarith only [l_gt_zero])\n    rcases h with ⟨n₀, hn₀⟩; use n₀\n    intro n hn; specialize hn₀ n hn\n    rw [lt_min_iff] at hn₀\n    exact hn₀.left\n  · exact fun h ε hε _ ↦ h ε hε\n* commutes with tendsTo. [TODO] I failed to finish the proof swiftly. You are welcome to optimize it!\ntheorem tendsTo_mul {a b : ℕ → ℝ} {A B : ℝ} (ha : TendsTo a A) (hb : TendsTo b B) :\n    TendsTo (fun n ↦ a n * b n) (A * B) := by\n  rw [← tendsTo_εlt_iff_TendsTo (show 1 &gt; 0 by linarith)]\n  intro ε hε hεlt1; simp\n  specialize ha (ε / (3 * (|B| + 1))) (by\n    apply div_pos hε\n    linarith only [abs_nonneg B])\n  rcases ha with ⟨n₁, ha⟩\n  specialize hb (ε / (3 * (|A| + 1))) (by\n    apply div_pos hε\n    linarith only [abs_nonneg A])\n  rcases hb with ⟨n₂, hb⟩\n  use max n₁ n₂\n  intro n hn\n  rw [max_le_iff] at hn\n  specialize ha n hn.left\n  specialize hb n hn.right\n  rw [show a n * b n - A * B = (a n - A) * (b n - B) + A * (b n - B) + B * (a n - A) by ring]\n  repeat grw [abs_add]\n  repeat grw [abs_mul]\n  grw [ha, hb]\n  -- sometimes you have no choice but add some manual steps\n  have h1 : |A| * (ε / (3 * (|A| + 1))) &lt; ε / 3 := by\n    field_simp\n    rw [div_lt_iff₀]\n    · ring_nf\n      linarith only [hε]\n    · linarith only [abs_nonneg A]\n  have h2 : |B| * (ε / (3 * (|B| + 1))) &lt; ε / 3 := by\n    field_simp\n    rw [div_lt_iff₀]\n    · ring_nf\n      linarith only [hε]\n    · linarith only [abs_nonneg B]\n  have h3 : ε / (3 * (|B| + 1)) * (ε / (3 * (|A| + 1))) &lt; ε / 3 := by\n    field_simp\n    rw [div_lt_iff₀]\n    · repeat grw [← abs_nonneg]\n      ring_nf\n      calc\n        _ = ε * ε := by ring\n        _ ≤ 1 * ε := by grw [← hεlt1]\n        _ = ε     := by ring\n        _ &lt; ε * 3 := by linarith only [hε]\n    · repeat grw [← abs_nonneg]\n      ring_nf\n      linarith only\n  linarith only [h1, h2, h3]\nsqueeze theorem for sequences\ntheorem tendsTo_sandwich {a b c : ℕ → ℝ} {L : ℝ} (ha : TendsTo a L) (hc : TendsTo c L)\n    (hab : ∀ n, a n ≤ b n) (hbc : ∀ n, b n ≤ c n) : TendsTo b L := by\n  unfold TendsTo\n  intro ε hε\n  specialize ha ε hε\n  specialize hc ε hε\n  rcases ha with ⟨n₀, hn₀⟩\n  rcases hc with ⟨m₀, hm₀⟩\n  use max n₀ m₀\n  intro n hn\n  rw [max_le_iff] at hn\n  specialize hab n\n  specialize hn₀ n (by linarith only [hn.left])\n  specialize hm₀ n (by linarith only [hn.right])\n  specialize hbc n\n  rw [abs_lt] at hn₀ hm₀ ⊢\n  constructor\n  · linarith only [hn₀, hm₀, hbc, hab]\n  · linarith only [hn₀, hm₀, hbc, hab]\nconstant sequence tends to zero iff condition\ntheorem tendsTo_zero_iff_lt_ε {x : ℝ} : TendsTo (fun _ ↦ x) 0 ↔ (∀ ε &gt; 0, |x| &lt; ε) := by\n  constructor\n  · intro h ε hε\n    specialize h ε hε\n    rcases h with ⟨n₀, hn₀⟩\n    specialize hn₀ n₀ (by linarith only)\n    simp at hn₀; exact hn₀\n  · intro h\n    intro ε hε\n    specialize h ε hε\n    use 0\n    intro n hn\n    simp; exact h\n[EXR] zero sequence tends to x iff condition\ntheorem zero_tendsTo_iff_lt_ε {x : ℝ} : TendsTo (fun _ ↦ 0) x ↔ (∀ ε &gt; 0, |x| &lt; ε) := by\n  constructor\n  · intro h\n    unfold TendsTo at h; simp at h\n    intro ε hε\n    specialize h ε hε\n    rcases h with ⟨n₀, hn₀⟩\n    specialize hn₀ n₀ (by linarith only)\n    exact hn₀\n  · intro h\n    intro ε hε\n    use 0\n    intro n hn\n    simp\n    exact h ε hε\nuniqueness of limits\ntheorem tendsTo_unique (a : ℕ → ℝ) (s t : ℝ) (hs : TendsTo a s) (ht : TendsTo a t) : s = t := by\n  by_contra! hneq\n  have hstp : 0 &lt; |t - s| := by\n    rw [abs_pos]\n    contrapose! hneq\n    apply_fun fun x ↦ x + s at hneq\n    simp at hneq\n    symm\n    exact hneq\n  have hst := tendsTo_sub hs ht\n  simp at hst\n  rw [zero_tendsTo_iff_lt_ε] at hst\n  specialize hst |t - s| hstp\n  rw [abs_sub_comm] at hst\n  linarith only [hst]\n\ndef contAt (f : ℝ → ℝ) (x₀ : ℝ) : Prop :=\n  ∀ ε &gt; 0, ∃ δ &gt; 0, ∀ x, |x - x₀| &lt; δ → |f x - f x₀| &lt; ε\n\ndef cont (f : ℝ → ℝ) : Prop := ∀ x₀ : ℝ, contAt f x₀\ncontinuity of function composition\ndef contAt_comp {f g : ℝ → ℝ} {x₀ : ℝ} (hf : contAt f (g x₀)) (hg : contAt g x₀) :\n    contAt (f ∘ g) x₀ := by\n  intro ε hε\n  rcases hf ε hε with ⟨δf, hδf, hf⟩\n  rcases hg δf hδf with ⟨δg, hδg, hg⟩\n  use δg, hδg\n  intro x hx\n  simp only [Function.comp_apply]\n  specialize hg x hx\n  specialize hf (g x) hg\n  exact hf\n[EXR] continuity of function composition\ndef cont_comp {f g : ℝ → ℝ} (hf : cont f) (hg : cont g) : cont (f ∘ g) := by\n  intro x\n  exact contAt_comp (hf (g x)) (hg x)\n[EXR] continuity implies sequential continuity\ndef tendsTo_of_contAt {f : ℝ → ℝ} {x₀ : ℝ} (hf : contAt f x₀)\n    {a : ℕ → ℝ} (ha : TendsTo a x₀) : TendsTo (f ∘ a) (f x₀) := by\n  intro ε hε\n  rcases hf ε hε with ⟨δ, hδ, hδf⟩\n  specialize ha δ hδ\n  rcases ha with ⟨n₀, hn₀⟩\n  use n₀\n  intro n hn\n  specialize hn₀ n hn\n  specialize hδf (a n) hn₀\n  exact hδf\nThe uniform limit of a sequence of continuous functions is continuous.\ndef uconv (f : ℕ → ℝ → ℝ) (f₀ : ℝ → ℝ) : Prop :=\n  ∀ ε &gt; 0, ∃ N : ℕ, ∀ n ≥ N, ∀ x : ℝ, |f n x - f₀ x| &lt; ε\n\ntheorem cont_of_cont_of_uconv\n    (f : ℕ → ℝ → ℝ) (f_cont : ∀ n : ℕ, cont (f n))\n    (f₀ : ℝ → ℝ) (h_uconv : uconv f f₀) : cont f₀ := by\n  intro x₀ ε hε\n  rcases h_uconv (ε / 3) (by linarith only [hε]) with ⟨N, hN⟩\n  specialize hN N (by linarith)\n  rcases f_cont N x₀ (ε / 3) (by linarith only [hε]) with ⟨δ, hδ, hδf⟩\n  use δ, hδ\n  intro x hx\n  specialize hδf x hx\n  have hNx := hN x\n  have hNx₀ := hN x₀\n  -- brute force `linarith` argument\n  rw [abs_lt] at hNx hNx₀ hδf ⊢\n  constructor\n  all_goals linarith only [hNx, hNx₀, hδf]\nThe sequential definition of function continuity is equivalent to the epsilon-delta definition.\ndef contAt_seq (f : ℝ → ℝ) (x₀ : ℝ) : Prop :=\n  ∀ a : ℕ → ℝ, TendsTo a x₀ → TendsTo (f ∘ a) (f x₀)\n[TODO] I failed to solve it swiftly. You are welcome to optimize it!\ntheorem contAt_iff_seq (f : ℝ → ℝ) (x₀ : ℝ) :\n    contAt f x₀ ↔ contAt_seq f x₀ := by\n  constructor\n  · intro hf a ha\n    exact tendsTo_of_contAt hf ha\n  · contrapose\n    intro hnfcont hnfseq\n    unfold contAt at hnfcont\n    push_neg at hnfcont\n    -- construct a sequence `a n` tending to `x₀`\n    let a (n : ℕ) : ℝ := 1 / (n + 1)\n    have a_gt_zero (n : ℕ) : a n &gt; 0 := by simp [a]; linarith only\n    have a_TendsTo_zero : TendsTo a 0 := by\n      intro ε hε\n      use Nat.ceil (1 / ε) -- ceiling function\n      intro n hn\n      rw [Nat.ceil_le] at hn\n      simp\n      rw [abs_of_pos (a_gt_zero n)]\n      unfold a\n      rw [div_lt_comm₀ (by linarith only) hε]\n      linarith only [hn]\n    -- construct a diverging sequence `f x` with `x` tending to `x₀`\n    -- this requires us to extract `Type*` objects from an existence to form a function\n    -- may meet universe issues if done naively\n    -- we use `Classical.indefiniteDescription` here to extract such objects classically\n    rcases hnfcont with ⟨ε, hε, hnf⟩\n    let x_subtype (n : ℕ) := Classical.indefiniteDescription _ &lt;| hnf (a n) (a_gt_zero n)\n    let x (n : ℕ) : ℝ := (x_subtype n).val\n    have x_lt_a (n : ℕ) : |x n - x₀| &lt; a n := by\n      unfold x\n      exact (x_subtype n).property.left\n    have fx_diverge (n : ℕ) : |f (x n) - f x₀| ≥ ε := by\n      unfold x\n      exact (x_subtype n).property.right\n\n    have x_tendsTo_x₀ : TendsTo x x₀ := by\n      suffices TendsTo (fun n ↦ x n - x₀) 0 by\n        have h_add := tendsTo_add this (tendsTo_const x₀)\n        simp at h_add; exact h_add\n      refine tendsTo_sandwich (?_ : TendsTo (fun n ↦ -a n) 0) (?_ : TendsTo (fun n ↦ a n) 0) ?_ ?_\n      · haveI := tendsTo_neg a_TendsTo_zero\n        simp at this; exact this\n      · exact a_TendsTo_zero\n      all_goals\n        intro n\n        haveI := x_lt_a n\n        rw [abs_lt] at this\n        linarith only [this]\n    -- but it is said that all such sequences converge\n    haveI := hnfseq x x_tendsTo_x₀\n    rcases this ε hε with ⟨n₀, hn₀⟩\n    specialize hn₀ n₀ (by linarith only); simp at hn₀\n    specialize fx_diverge n₀\n    linarith only [hn₀, fx_diverge]",
    "crumbs": [
      "07-analysis.lean"
    ]
  },
  {
    "objectID": "06-ineq.html",
    "href": "06-ineq.html",
    "title": "06-ineq.lean",
    "section": "",
    "text": "import Mathlib",
    "crumbs": [
      "06-ineq.lean"
    ]
  },
  {
    "objectID": "06-ineq.html#inequality-1",
    "href": "06-ineq.html#inequality-1",
    "title": "06-ineq.lean",
    "section": "1 Inequality",
    "text": "1 Inequality\n\n1.1 Basics\nInequality is determined by a partial order PartialOrder. A partial order is a relation with reflexivity, antisymmetry, and transitivity. In Lean, a relation means α → α → Prop for some type α, capturing the fact that each a ≤ b gives a proposition.\nsection\n\nvariable (a b c d : ℚ)\nPartialOrder makes LE(≤) and LT(&lt;) available in the context.\n#check PartialOrder\n\n#check a ≤ b\n#check a &lt; b\n#check b ≥ a\n#check b &gt; a\n\n#check le_refl\n#check le_antisymm\n#check le_trans\n\n#check lt_irrefl\n#check lt_asymm\n#check lt_trans\n&lt; is determined by ≤\n#check lt_iff_le_not_ge\n≥, &gt; are just aliases of ≤, &lt;\nexample : (a &lt; b) = (b &gt; a) := by rfl\nexample : (a ≤ b) = (b ≥ a) := by rfl\n\nexample : a &lt; b ↔ a ≤ b ∧ a ≠ b := by\n  rw [lt_iff_le_not_ge]\n  constructor\n  · intro ⟨hab, hnba⟩\n    constructor\n    · exact hab\n    · intro h\n      rw [h] at hnba\n      apply hnba\n      exact le_refl b\n  · intro ⟨hab, hnab⟩\n    constructor\n    · exact hab\n    · intro hba\n      apply hnab\n      exact le_antisymm hab hba\n#check lt_of_le_of_ne -- this have a related theorem\nA linearly ordered commutative ring is a commutative ring with a total order s.t addition and multiplication are strictly monotone, e.g. ℚ.\nIn Lean this reads [CommRing R] [LinearOrder R] [IsStrictOrderedRing R].\nWe will work with ℚ as an example afterwards.\n[TODO] For some reason, LinearOrder ℚ is constructed using classical logic. Don’t be surprised if #print axioms ... shows some classical axioms.\n\n\n1.2 Pure partial order reasoning\nnorm_num tactic solves numerical equalities and inequalities automatically.\n#help tactic norm_num\nexample : (22 / 7 : ℚ) &lt; 4 := by norm_num\n\n-- [EXR]\nexample (hab : a ≤ b) (hba : b ≤ a) : a = b := by\n  apply le_antisymm\n  · exact hab\n  · exact hba\ngrw rewrites like rw, but works for inequalities.\n#help tactic grw\nexample (hab : a ≤ b) (hbc : b &lt; c) : a &lt; c := by\n  grw [hab]\n  exact hbc\nexample (hab : a ≤ b) (hbc : b &lt; c) : a &lt; c := by\n  grw [← hab] at hbc\n  exact hbc\n#check lt_of_le_of_lt -- this have a name\ncalc is a term / tactic for proving inequalities by chaining.\n#help tactic calc\nexample (hab : a ≤ b) (hbc : b &lt; c) : a &lt; c := by\n  calc\n    a ≤ b := hab\n    _ &lt; c := hbc\n\n\n1.3 Linear order reasoning\nA linear order is a partial order with le_total: either a ≤ b or b ≤ a.\n#check le_total\n[EXR] Use this to prove the trichotomy of &lt; and =.\nexample : a &lt; b ∨ a = b ∨ a &gt; b := by\n  rcases le_total a b with (hle | h)\n  · by_cases heq : a = b\n    · right; left; exact heq\n    · left\n      apply lt_of_le_of_ne\n      · exact hle\n      · exact heq\n  · -- do it similarly\n    sorry\n#check eq_or_lt_of_le -- this have a name\n\n\n1.4 Monotonicity of +\nIt’s important to recognize that the (strict) monotonicity of + is a nontrivial theorem. That is a part of the meaning of IsStrictOrderedRing.\n#synth IsStrictOrderedRing ℚ\n\n#check add_le_add_left\n#check add_le_add_right\n\n#check add_lt_add_left\n#check add_lt_add_right\nLuckily, grw recognizes these theorems and applies them automatically.\ntransposition of ≤\nexample (h : a + b ≤ c) : a ≤ c - b := by\n  grw [← h]\n  simp\nmonotonicity of +\nexample : a + c ≤ b + c ↔ a ≤ b := by\n  constructor\n  · intro h\n    calc\n      a = (a + c) - c := by simp\n      _ ≤ (b + c) - c := by grw [h]\n      _ = b := by simp\n  · intro h\n    grw [h]\n#check add_le_add_iff_right -- this have a name\nstrict monotonicity of +\nexample : a &lt; b ↔ a + c &lt; b + c := by\n  constructor\n  · contrapose!\n    intro h\n    rw [add_le_add_iff_right] at h\n    exact h\n  · contrapose!\n    intro h\n    grw [h]\n#check add_lt_add_iff_right -- this have a name\n\n-- [EXR]\nexample (h : a + b &lt; c + d) : a - d &lt; c - b := by\n  sorry\n\n\n1.5 Automation\nTired of these? Use automation!\n\nlinarith\nlinarith is a powerful tactic that solves linear inequalities automatically. It uses hypotheses in the context and basic properties of linear orders to deduce the goal.\nlinarith only [h1, h2, ...] use only hypotheses h1, h2, … to solve the goal.\n#help tactic linarith\n\nexample : a &lt; b ↔ a - c &lt; b - c := by\n  constructor\n  all_goals\n    intro\n    linarith\n\nexample (h : a + b &lt; c + d) : a - d &lt; c - b := by\n  linarith\n\nexample (h : a &gt; 0) : (2 / 3) * a &gt; 0 := by\n  linarith\n\nexample (h : (-5 / 3) * a &gt; 0) : 4 * a &lt; 0 := by\n  linarith\nNote the limitations of linarith.\nIt only works for linear inequalities, not polynomial ones.\nexample : a ^ 2 ≥ 0 := by sorry -- linarith fails here\nthough some of polynomial inequalities can be solved by nlinarith\n#help tactic nlinarith\nexample : a ^ 2 ≥ 0 := by nlinarith\n#check sq_nonneg -- this have a name\nIt solve all inequalities in a dense linear order.\nIt does solve some inequalities in discrete linear orders like ℤ, but no guarantee for all of them.\nexample (n m : ℤ) (h : n &lt; m) : n + 1 ≤ m := by linarith\nexample (n m : ℤ) (h : n &lt; m) : n + (1/2 : ℚ) ≤ m := by sorry -- linarith fails here\nIt won’t recognize inequalities involving min, max, abs, etc. It won’t recognize some basic simp transformations, either.\nexample (h : a * (min 1 2) &gt; 0) : (id a) ≥ 0 := by\n  simp at *\n  linarith -- direct `linarith` will fail\n[EXR] ℚ admits a dense linear order\nexample (hab : a &lt; b) : ∃ c : ℚ, a &lt; c ∧ c &lt; b := by\n  use (a + b) / 2\n  constructor\n  all_goals linarith\n\n\nsimp\nadd_lt_add_iff_right-like theorems are registered for simp, so sometimes simp can reduce things like:\nexample (h : a + b &lt; c + b) : a &lt; c := by\n  simp at h\n  exact h\n\n\napply_fun\nSometimes you would like to apply_fun at an inequality. This requires you to manually show the monotonicity of the function.\nexample (h : a + b &lt; c + d) : a - d &lt; c - b := by\n  apply_fun (· - b - d) at h\n  · ring_nf at *\n    exact h\n  · unfold StrictMono\n    simp\n[EXR] Mimick the above example.\nexample (h : a + c ≤ b) : a ≤ b - c := by\n  apply_fun (· - c) at h\n  · ring_nf at *\n    exact h\n  · unfold Monotone\n    simp\n\n\n\n1.6 Monotonicity of *\n[TODO] It’s not needed in the course so far, so we skip it for now.\nend",
    "crumbs": [
      "06-ineq.lean"
    ]
  },
  {
    "objectID": "06-ineq.html#abs-min-max-and-taocp",
    "href": "06-ineq.html#abs-min-max-and-taocp",
    "title": "06-ineq.lean",
    "section": "2 abs, min, max and TAOCP",
    "text": "2 abs, min, max and TAOCP\nA mature formalizer finds their theorems by themselves. The art of capturing premises includes, but not limited to:\n\nexact?\nname guessing\nnatural language search engine: LeanSearch, LeanExplore, etc.\nmathlib documentation\nAI copilot completion\n\nsection\n\nvariable (a b c : ℚ)\n\n#check abs\n[EXR] Find all the below by yourself\nexample : |a| ≥ 0 := by exact abs_nonneg a\nexample : |-a| = |a| := by exact abs_neg a\nexample : |a * b| = |a| * |b| := by exact abs_mul a b\nexample : |a + b| ≤ |a| + |b| := by exact abs_add_le a b\nexample : |a| - |b| ≤ |a - b| := by exact abs_sub_abs_le_abs_sub a b\nexample : |a| ≤ b ↔ -b ≤ a ∧ a ≤ b := by exact abs_le\nexample : |a| ≥ b ↔ a ≤ -b ∨ b ≤ a := by exact le_abs'\n\nexample (h : a ≥ 0) : |a| = a := by exact abs_of_nonneg h\nexample (h : a ≤ 0) : |a| = -a := by exact abs_of_nonpos h\nexample (h : b ≥ 0) : |a| = b ↔ a = b ∨ a = -b := by exact abs_eq h\nA mindless way to prove these linear inequalities involving abs is to eliminate all abs by casing on the sign of the arguments, then use linarith.\nexample : |a - c| ≤ |a - b| + |b - c| := by\n  all_goals rcases le_total 0 (a - b) with h1 | h1\n  all_goals\n    try rw [abs_of_nonneg h1]\n    try rw [abs_of_nonpos h1]\n  all_goals rcases le_total 0 (b - c) with h2 | h2\n  all_goals\n    try rw [abs_of_nonneg h2]\n    try rw [abs_of_nonpos h2]\n  all_goals rcases le_total 0 (a - c) with h3 | h3\n  all_goals\n    try rw [abs_of_nonneg h3]\n    try rw [abs_of_nonpos h3]\n\n  all_goals linarith\ncombine brute-force method with theorem-finding\nexample : |(|a| - |b|)| ≤ |a - b| := by\n  rcases le_total 0 (|a| - |b|) with h1 | h1\n  all_goals\n    try rw [abs_of_nonneg h1]\n    try rw [abs_of_nonpos h1]\n  · apply abs_sub_abs_le_abs_sub\n  · simp only [neg_sub] -- use `simp only` to supress unwanted lemmas\n    grw [abs_sub_abs_le_abs_sub]\n    rw [← abs_neg]\n    simp\n\n-- [EXR]\nexample : |(|a| - |b|)| ≤ |a + b| := by\n  rcases le_total 0 (|a| - |b|) with h1 | h1\n  all_goals\n    try rw [abs_of_nonneg h1]\n    try rw [abs_of_nonpos h1]\n  · haveI := abs_sub_abs_le_abs_sub a (-b)\n    simp at *\n    exact this\n  · haveI := abs_sub_abs_le_abs_sub b (-a)\n    simp at *\n    grw [this]\n    ring_nf\n    simp\n[EXR] Get familiar with min, max and solve the following by yourself.\nexample : min a b ≤ a := by exact min_le_left a b\nexample : min a b + max a b = a + b := by exact min_add_max a b\n\nend",
    "crumbs": [
      "06-ineq.lean"
    ]
  },
  {
    "objectID": "06-ineq.html#wheelchair-tactics",
    "href": "06-ineq.html#wheelchair-tactics",
    "title": "06-ineq.lean",
    "section": "3 Wheelchair tactics",
    "text": "3 Wheelchair tactics\nYou have seen some all-in-one tactics like simp, ring and linarith. There are even more powerful tactics that save your effort. Do try them when you feel tired of trivial steps.\n#help tactic simp\n#help tactic dsimp\n#help tactic simp_rw\n#help tactic field_simp\n\n#help tactic group\n#help tactic abel\n#help tactic ring\n#help tactic module\n\n#help tactic linarith\n#help tactic nlinarith\n\n#help tactic omega\n#help tactic aesop\n#help tactic grind\n#help tactic tauto\nA tactic cheatsheet is available at lean-tactics.pdf",
    "crumbs": [
      "06-ineq.lean"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "Introduction to Formal Mathematics with Lean 4",
    "section": "",
    "text": "What is formalization\n\nNatural language vs. formal language\n\nambiguity in natural language\n\nimplicit assumptions\nskipping details: “It’s clear that we have…”\n“viewed as” arguments: \\(V^{**} = V\\), \\((A \\times B) \\times C = A \\times (B \\times C)\\) 1\nabuses of notation: \\(3 \\in \\mathbb Z / 5 \\mathbb Z\\), \\(\\mathbb C \\subseteq \\mathbb C[x]\\)\n\nprecision in formal language\n\ncomputer programs are formal languages\n\n\n1 Knowledgable audience may recognize them as examples of natural isomorphisms in category theory.\n\nMathematical proofs vs. Computer programs2\n\nCurry–Howard correspondence\n\n\nLogic\nProgramming\n\n\n\n\nproposition\ntype\n\n\nproof\nterm\n\n\nproposition is true\ntype has a term\n\n\nproposition is false\ntype doesn’t have a term\n\n\nlogical constant TRUE\nunit type\n\n\nlogical constant FALSE\nempty type\n\n\nimplication \\(\\to\\)\nfunction type\n\n\nconjunction \\(\\land\\)\nproduct type \\(\\prod\\)\n\n\ndisjunction \\(\\lor\\)\nsum type \\(\\sum\\)\n\n\nuniversal quantification \\(\\forall\\)\ndependent product type \\(\\prod\\)\n\n\nexistential quantification \\(\\exists\\)\ndependent sum type \\(\\sum\\)\n\n\n\n\n2 see also Computational Trilogy, with category theory as the third vertex\nSet theory vs. Type theory\n\nMathematicians choose axiomatic set theory (with first-order logic) as the foundation of mathematics.\n\nnaive set theory fits human’s intuition well\n\nType theory is an alternative foundation that is equally expressive, but more suitable for computer formalization.\n\n\n\n\nSet Theory\nType Theory\n\n\n\n\neverything is a set\neverything has a type\n\n\n\\(3 \\in \\mathbb R\\) is a proposition\n\\((3 : \\mathbb R)\\) is a typing judgment\n\n\n\\(\\mathbb Q \\subseteq \\mathbb R\\) is an inclusion\n\\(\\mathbb Q \\to \\mathbb R\\) is a type conversion\n\n\n\n\n\nWhat is Lean 4\n\nA modern functional programming language designed for theorem proving\n\n\n“Lean is based on a version of dependent type theory known as the Calculus of Constructions, with a countable hierarchy of non-cumulative universes and inductive types.” — Theorem Proving in Lean 4\n\n\n\nLean’s dependent type theory\n\nDependent type theory is a powerful extension of type theory where\n\ntypes may depend on terms “given before” them\nfirst-order logic can be implemented in dependent type theory\n\nfunctions, inductive types and quotient types3 are the basic methods to construct new types.\n\n3 Though seemingly redundant, there are reasons for making quotient types as a fundamental constructing method. funext thesis\n\n\nSet Theory\nLean’s dependent type theory\n\n\n\n\n\\(\\forall x \\in \\mathbb R,\\, x^2 \\geq 0\\)\nhas type \\((x : \\mathbb R) \\to (x^2 \\geq 0)\\)\n\n\n\\((n \\in \\mathbb N) \\mapsto (1,0,\\dots,0) \\in \\mathbb R^n\\)\nhas type \\((n : \\mathbb N) \\to \\mathbb R^n\\)\n\n\n\\(\\{ 0,1 \\} = 2\\) is a set equality\nmake no sense\n\n\ncardinality is an equivalence class\nis a quotient type\n\n\nRussell’s paradox\nGirard’s paradox\n\n\n\n\n\nAn example Lean 4 code\n\nFLT\nTendsTo\n\n\n\n\nWhy formalize\n\nThe rise of AI\nAI excels in Python. Why not Lean?\n\nAutomated theorem proving\n\nespecially those “abstract nonsense”\nfull-auto (create a proof without human interaction)\nsemi-auto (suggest tactics)\n\nexact?, Github Copilot, …\n\n\nNatural language to formal language\n\nautomatically transplanting textbooks and papers into Lean\nfull-auto (translate without human interaction)\nbolt-action (search for existing theorems)\n\nLeanSearch, LeanExplore, …\n\nConverse? Already happening!\n\nProposing conjectures\n\non which facts should we care about\n\n\n\n\nRigor matters\n\nIt’s the foundation of mathematics\nImprecise natural language often leads to misunderstandings and glitches\n\nEspecially when proofs get longer and longer\n\nformalization fully confirms the correctness of a theorem\n\nthings that are too “technical” (boring) or simply impossible to verify by oneself\n\ne.g. classification of finite simple groups\ne.g. “technical”\n\n\n“I spent much of 2019 obsessed with the proof of this theorem, almost getting crazy over it. In the end, we were able to get an argument pinned down on paper, but I think nobody else has dared to look at the details of this, and so I still have some small lingering doubts.” — Peter Scholze\n\n\n\n\n\n“Mathematical engineering”\n\nmanipulating tons of theorems and proofs with mature software engineering techniques\nreferencing existing theorems as dependencies\ncollaborative work across the globe\n\n\nThe beauty of the system: you do not have to understand the whole proof of FLT in order to contribute. The blueprint breaks down the proof into many many small lemmas, and if you can formalise a proof of just one of those lemmas then I am eagerly awaiting your pull request. — Kevin Buzzard on the FLT Project\n\n\n\nFormalization as learning\n\nproofs with infinite detail\n\nintuitive textbooks, rigorous formalization\n\nmakes us understand things better\n\nGlobal: How to build natural numbers from scratch?\n\nnatural number game\nA journey to the world of numbers, by Riccardo Brasca\n\nLocal: reducing the cognitive load\n\n(With good organization at the beginning) you can focus on small parts of the proof at a time\n\n\n\n\n\n\nWhy now formalize\n\nFormalization becomes more accessible\nMathematician-friendly languages, interfaces, tools and community emerges:\n\nLean 4 with VSCode extension, modern interactive theorem prover made for mathematicians\nformalization becomes more and more fashionable\nbig names works on formalization:\n\nKevin Buzzard works on formalizing FLT\nPeter Scholze’s work on condensed mathematics has been formalized\nTerrence Tao gave a talk on formalization in IMO 2024 and wrote a Lean 4 companion of his book “Analysis I” recently\n\ncomputer scientists and volunteering mathematicians run Lean 4 community collaboratively\n\n\n\nMathlib 4 is expanding explosively\nBy the time of 2025/09/16, Mathlib 4 has4\n4 Statistics fetched from Mathlib statistics\n\n\nLines of code\nDefinitions\nTheorems\nContributors\n\n\n\n\n1950000\n115438\n232204\n653\n\n\n\n\nundergraduate may contribute: some low-hanging fruits\n\n\n\n\nHow to formalize\n\nThe goal\nThe goal, at the end of this course, is\n\nto get used to think formally\nto migrate from set theory to dependent type theory\nto practice basic skills to translate statements and proofs into Lean 4\nto know how to find existing theorems, how the community works\nto acquire enough common senses to read the bibles (MiL, TPiL, Mathlib 4 Doc, etc.) by yourself for future formalization projects\n(optimistically) to set up a Lean 4 formalization club at BIT!\n\n\n\nHow will we learn\nAs mathematicians, we learn Lean 4 to formalizing mathematics. We learn by practice.\n\nDozens of Lean files packed with well-organized examples and exercises suffice to get you started, suitable for both guided study and self study. Most lectures will be given in this style.\nThis style of teaching is inspired by Kevin Buzzard’s 2024 course on formalising mathematics in the Lean theorem prover and many other courses.\n\n\n\nWhat we won’t cover\nDue to the limited time, my personal inability and the design of this course, we might not be able to cover:\n\na deep discussion into dependent type theory or Lean as a programming language itself\n\nRead TPiL for a Lean 4 tutorial that emphasizes on type theory.\nRead FPiL for a Lean 4 tutorial that focuses more on functional programming.\nRefer to Lean Language Manual for precise specifications.\n\nsystematic exposition of how a particular branch of mathematics is formalized in Mathlib 4\n\nRead MiL for this purpose.\n\nhow to organize a massive formalization project from scratch, i.e. project management\n\nsomewhat subtle, might can only be learned by reading Mathlib codes and practical experience\n\n\n\n\nDisclaimer\n\nFormalization is tedious in its nature, Lean is no exception\nType conversions can be an extra burden (exclusive for type-theory-based systems)\nKnowledge needs to be re-learned before being referenced\nDifferent people may formalize the same thing in different ways\n\n\n\nIf these do not scare you away…\nWelcome aboard. Have fun formalizing mathematics!\n\n\nResources\n\ncourse repository\nonline documentation",
    "crumbs": [
      "Introduction to Formal Mathematics with Lean 4"
    ]
  },
  {
    "objectID": "09-subgroup.html",
    "href": "09-subgroup.html",
    "title": "09-subgroup.lean",
    "section": "",
    "text": "import Mathlib",
    "crumbs": [
      "09-subgroup.lean"
    ]
  },
  {
    "objectID": "09-subgroup.html#subsets",
    "href": "09-subgroup.html#subsets",
    "title": "09-subgroup.lean",
    "section": "1 Subsets",
    "text": "1 Subsets\n\n1.1 Objects\nIn the previous lectures, we regarded types as sets intuitively. This is not flexible when one wishes to restrict to only a fraction of the elements of a type. It’s also hard to implement unions and intersections of sets this way. Mathlib provides a dedicated type Set α, consists of all the subsets of a type α.\nsection\n\nvariable (α : Type*) (s t u : Set α) (a : α)\n\n#print Set\nYou can see that Set α is defined as α → Prop.\nThis means a subset s : Set α tells you, for each a : α, whether a belongs to s or not. This proposition is denoted by a ∈ s, Set.mem s a, or s.Mem a.\nNote that you are not supposed to write s a directly. The function definition of Set α should be regarded as an implementation detail.\n#check a ∈ s\n#check s.Mem a\nA subset can be constructed using the set-builder notation {x : α | p x} or setOf p, where p : α → Prop is a predicate on α.\n#check setOf (fun x ↦ x = a)\nexample : Set α := {x : α | x = a} \nThe same as above\nexample : Set α := Set.singleton a \nThe same as above\nexample : Set ℕ := {n | n &gt; 514}\n\n#check (∅ : Set α) \nThe empty subset\nexample : ∅ = {x : α | False} := by rfl\nexample : a ∈ (∅ : Set α) ↔ False := by rfl\n#check Set.mem_empty_iff_false -- corresponding [@simp] lemma\n\n#check (Set.univ : Set α) \nThe universal subset\nexample : Set.univ = {x : α | True} := by rfl\nexample : a ∈ Set.univ := by trivial\n#check Set.mem_univ -- corresponding [@simp] lemma\n\n#check sᶜ \nThe complement of a subset Set.compl s\nexample : sᶜ = {x | x ∉ s} := by rfl\nexample : a ∈ sᶜ ↔ a ∉ s := by rfl\n#check Set.mem_compl -- corresponding [@simp] lemma\n#check rfl\n\n#check s ⊆ t \nSubset relation Set.Subset s t\nexample : s ⊆ t ↔ ∀ x : α, x ∈ s → x ∈ t := by rfl\nexample (ha : a ∈ s) (hst : s ⊆ t) : a ∈ t := hst ha\n\n#check s ∩ t \nIntersection of two subsets Set.inter s t\nexample : a ∈ s ∩ t ↔ a ∈ s ∧ a ∈ t := by rfl\n\n#check s ∪ t \nUnion of two subsets Set.union s t\nexample : a ∈ s ∪ t ↔ a ∈ s ∨ a ∈ t := by rfl\next tactic reduces subset equality to element membership. Fundamentally this is implimented using function & propositional extensionality.\n#check Set.ext\nexample : s ∩ t = t ∩ s := by ext x; simp [and_comm]\n[EXR] De Morgan’s law for sets\nexample : s ∩ (t ∪ u) = (s ∩ t) ∪ (s ∩ u) := by tauto_set\n#help tactic tauto_set -- Wheelchair tactic for set equations\n\nend\n\n\n1.2 Morphisms\nFunctions between types induce functions between subsets.\nsection\n\nvariable {α β : Type*} (f : α → β) (s : Set α) (t : Set β) (a : α) (b : β)\n\n#check Set.range f\nexample : Set.range f = {y | ∃ x, f x = y} := by rfl\nexample : b ∈ Set.range f ↔ ∃ x, f x = b := by rfl\n#check Set.mem_range -- corresponding [@simp] lemma\n\n#check f '' s \nImage of a subset Set.image f s\n#check Set.image f s\nexample : f '' s = {y | ∃ x ∈ s, f x = y} := by rfl\nexample : f '' s = {f x | x ∈ s} := by rfl -- set-builder notation for image\nexample : b ∈ f '' s ↔ ∃ x ∈ s, f x = b := by rfl\n#check Set.mem_image -- corresponding [@simp] lemma\n\n#check f ⁻¹' t \nPreimage of a subset Set.preimage f t\n#check Set.preimage f t\nexample : f ⁻¹' t = {x | f x ∈ t} := by rfl\nexample : a ∈ f ⁻¹' t ↔ f a ∈ t := by rfl\n#check Set.mem_preimage -- corresponding [@simp] lemma\nNote the following is not a definitional equality. The last step invokes propext, which destroys definitional equality. It has some unfortunate consequences in later discussions, when additional structure is involved.\nexample : f '' Set.univ = Set.range f := by\n  ext x\n  rw [Set.mem_range, Set.mem_image]\n  simp only [Set.mem_univ, true_and]\n#check Set.image_univ -- corresponding [@simp] lemma\nThe so-called Galois connection between image and preimage.\nexample : f '' s ⊆ t ↔ s ⊆ f ⁻¹' t := by\n  constructor\n  · intro h x hx\n    simp only [Set.mem_preimage]\n    apply h\n    simp only [Set.mem_image]\n    use x\n  · intro h y hy\n    rcases hy with ⟨x, hxs, hxy⟩\n    specialize h hxs\n    simp only [Set.mem_preimage] at h\n    rw [hxy] at h; exact h\n#check Set.image_subset_iff -- corresponding [@simp] lemma\nWe recall some definitions about functions.\n#check Function.comp\n#check Function.Injective\n#check Function.Surjective\n#check Function.Bijective\nFor those seeking a more challenging exercise, try proving the Bernstein–Schroeder theorem. See MiL chapter 3 for an answer.\n#check Function.Embedding.schroeder_bernstein\n\nend",
    "crumbs": [
      "09-subgroup.lean"
    ]
  },
  {
    "objectID": "09-subgroup.html#subsemigroups",
    "href": "09-subgroup.html#subsemigroups",
    "title": "09-subgroup.lean",
    "section": "2 Subsemigroups",
    "text": "2 Subsemigroups\n\n2.1 Objects\nA Subsemigroup G is a subset of a Semigroup G that is closed under the multiplication.\nIt’s actually a bundled structure consisting of a subset and a proof of closure. To use it like a subset, Mathlib registers Subsemigroup G as an instance of SetLike G. It provides coercion from Subsemigroup G to Set G, so for H : Subsemigroup G, you can use a ∈ H to mean a belongs to the underlying subset of H.\nsection\n\nvariable (G : Type*) [Semigroup G]\nvariable (H₁ H₂ : Subsemigroup G) (a b : G)\nexample (ha : a ∈ H₁) (hb : b ∈ H₁) : a * b ∈ H₁ := mul_mem ha hb\nthe whole semigroup as a subgroup\n#check (⊤ : Subsemigroup G)\nexample : (⊤ : Subsemigroup G) = ⟨Set.univ, by simp⟩ := by rfl\n#synth Top (Subsemigroup G)\nthe empty subset as a subgroup\n#check (⊥ : Subsemigroup G)\nexample : (⊥ : Subsemigroup G) = ⟨∅, by simp⟩ := by rfl\n#synth Bot (Subsemigroup G)\nThe partial order structure on subsemigroups is inherited from subset relation of subsets.\nexample : H₁ ≤ H₂ ↔ H₁.carrier ⊆ H₂.carrier := by rfl\nintersection of two subsemigroups\n#check H₁ ⊓ H₂\n#synth Min (Subsemigroup G)\n\nexample : H₁ ⊓ H₂ = ⟨H₁ ∩ H₂, by\n    intro a b ha hb\n    rcases ha with ⟨ha₁, ha₂⟩\n    rcases hb with ⟨hb₁, hb₂⟩\n    constructor\n    all_goals apply mul_mem\n    all_goals assumption\n    ⟩ :=\n  rfl\nproduct of two subsemigroups.\n#check H₁ ⊔ H₂\n#synth Max (Subsemigroup G)\nDefinition of H₁ ⊔ H₂ is more involved, relying the lattice structure of Subsemigroup G. it is defined as the infimum of all subsemigroups containing both H₁ and H₂, where the infimum is given by intersection.\n#synth CompleteLattice (Subsemigroup G)\nThis is characterized by the following properties.\n#synth SemilatticeSup (Subsemigroup G)\nexample : H₁ ≤ H₁ ⊔ H₂ := by apply le_sup_left\nexample : H₂ ≤ H₁ ⊔ H₂ := by apply le_sup_right\nexample (K : Subsemigroup G) (h1 : H₁ ≤ K) (h2 : H₂ ≤ K) : H₁ ⊔ H₂ ≤ K :=\n  sup_le h1 h2\n\nend\n\n\n2.2 Morphisms\nLet’s see how MulHom interacts with Subsemigroup.\nsection\n\nvariable {G₁ G₂ : Type*} [Semigroup G₁] [Semigroup G₂]\n         (f : G₁ →ₙ* G₂) (H₁ : Subsemigroup G₁) (H₂ : Subsemigroup G₂)\nThe image of a subsemigroup under a MulHom is also a subsemigroup.\n#check Subsemigroup.map f H₁\n#check H₁.map f\n\nexample : Subsemigroup.map f H₁ = ⟨f '' H₁, by\n    rintro x y ⟨a, ha, rfl⟩ ⟨b, hb, rfl⟩\n    use a * b, H₁.mul_mem ha hb\n    rw [map_mul]\n    ⟩ :=\n  rfl\nThe preimage of a subsemigroup under a MulHom is also a subsemigroup.\n#check Subsemigroup.comap\n#check H₂.comap f\n\nexample : Subsemigroup.comap f H₂ = ⟨f ⁻¹' H₂, by\n    intro x y hx hy\n    simp only [Set.mem_preimage] at hx hy ⊢\n    rw [map_mul]\n    exact H₂.mul_mem hx hy\n    ⟩ :=\n  rfl\nTo define the range of a f : G₁ →ₙ* G₂, a common idea is to adopt (⊤ : Subsemigroup G₁).map f. Unfortunately, this makes the underlying set being f '' (univ : Set G₁), which is not definitionally equal to Set.range f. It will also cause x ∈ ⊤ conditions in later proofs, redundant and annoying.\nHencee Mathlib define the range with some refinement: They manually replace the underlying set of (⊤ : Subsemigroup G₁).map f with Set.range f.\nSee Note range copy pattern for an official explanation.\n#check MulHom.srange\nthe desired definitional equality\nexample : MulHom.srange f = ⟨Set.range f, by\n    rintro x y ⟨a, rfl⟩ ⟨b, rfl⟩\n    use a * b\n    rw [map_mul]\n    ⟩ := by rfl\n\nexample (x : G₂) : x ∈ MulHom.srange f ↔ x ∈ Set.range f := by rfl\n#check MulHom.mem_srange -- corresponding Mathlib theorem\n\nend",
    "crumbs": [
      "09-subgroup.lean"
    ]
  },
  {
    "objectID": "09-subgroup.html#submonoids",
    "href": "09-subgroup.html#submonoids",
    "title": "09-subgroup.lean",
    "section": "3 Submonoids",
    "text": "3 Submonoids\nA Submonoid M is a subsemigroup of a Monoid M that contains the identity element.\nsection\n\nvariable (G : Type*) [Monoid G]\nvariable (H₁ H₂ : Submonoid G) (a b : G)\n\nexample : a ∈ H₁ → b ∈ H₁ → a * b ∈ H₁ := by apply mul_mem\nexample : (1 : G) ∈ H₁ := by apply one_mem\nThe whole monoid as a submonoid. Note the use of with, to extend the underlying Subsemigroup with the proof of containing 1.\n#check (⊤ : Submonoid G)\nexample : (⊤ : Submonoid G) = {(⊤ : Subsemigroup G) with\n    one_mem' := by\n      change 1 ∈ (⊤ : Set G)\n      apply Set.mem_univ\n    } := by rfl\n#synth Top (Submonoid G)\nThe trivial submonoid consisting of only the identity element. Note the difference from ⊥ : Subsemigroup G, which is the empty set.\n#check (⊥ : Submonoid G)\nexample : (⊥ : Submonoid G) = {\n    carrier := {1}\n    one_mem' := by rfl\n    mul_mem' := by\n      rintro x y hx hy\n      simp only [Set.mem_singleton_iff] at hx hy ⊢\n      rw [hx, hy, one_mul]\n    } := by rfl\n#synth Bot (Submonoid G)\nWe don’t repeat tedious lattice structure part, which is similar to those for Subsemigroup.\n#synth CompleteLattice (Submonoid G)\n\nend\n\n3.1 Morphisms\nMonoidHom interacts with Submonoid similarly to MulHom and Subsemigroup.\nsection\n\nvariable {G₁ G₂ : Type*} [Monoid G₁] [Monoid G₂]\n         (f : G₁ →* G₂) (H₁ : Submonoid G₁) (H₂ : Submonoid G₂)\nWe still have image and preimage of submonoids, which can be built on top of those for subsemigroups, with extra care to verify the identity element membership.\n#check Submonoid.map\nexample : Submonoid.map f H₁ = { Subsemigroup.map f.toMulHom H₁.toSubsemigroup with\n      one_mem' := by\n        simp\n        use 1, H₁.one_mem\n        rw [map_one]\n    } := by rfl\n\n#check Submonoid.comap\nexample : Submonoid.comap f H₂ = { Subsemigroup.comap f.toMulHom H₂.toSubsemigroup with\n      one_mem' := by simp\n    } := by rfl\nRange is also specially handled as MulHom.range.\n#check MonoidHom.mrange\nexample (x : G₂) : x ∈ MonoidHom.mrange f ↔ x ∈ Set.range f := by rfl\n#check MonoidHom.mem_mrange -- corresponding Mathlib theorem\nWith the presence of identity element, we can define the kernel of a MonoidHom.\n#check MonoidHom.mker\nexample : MonoidHom.mker f = (⊥ : Submonoid G₂).comap f := by rfl\n[EXR] manual definition of mker\nexample : MonoidHom.mker f = {\n      carrier := {x | f x = 1}\n      one_mem' := by rw [Set.mem_setOf, map_one]\n      mul_mem' := by\n        rintro x y hx hy\n        simp only [Set.mem_setOf] at hx hy ⊢\n        rw [map_mul, hx, hy, one_mul]\n    } := by rfl\nexample (x : G₁) : x ∈ MonoidHom.mker f ↔ f x = 1 := by rfl\n#check MonoidHom.mem_mker -- corresponding Mathlib theorem\n\nend\n\n\n3.2 Exercise\nAs an exercise, let’s define addition on AddSubmonoid A with the intrinsic definition, and show that it coincides with the supremum.\nsection\n\nvariable {A : Type*} [AddCommMonoid A]\n\ninstance : Add (AddSubmonoid A) := ⟨fun B₁ B₂ ↦ {\n  carrier := {x | ∃ b₁ ∈ B₁, ∃ b₂ ∈ B₂, x = b₁ + b₂}\n  zero_mem' := ⟨0, B₁.zero_mem, 0, B₂.zero_mem, by rw [add_zero]⟩\n  add_mem' := by\n    rintro x y ⟨b₁, hb₁, b₂, hb₂, rfl⟩ ⟨c₁, hc₁, c₂, hc₂, rfl⟩\n    use b₁ + c₁, B₁.add_mem hb₁ hc₁\n    use b₂ + c₂, B₂.add_mem hb₂ hc₂\n    abel\n}⟩\n\nexample (B₁ B₂ : AddSubmonoid A) : B₁ ⊔ B₂ = B₁ + B₂ := by\n  apply le_antisymm\n  · apply sup_le\n    · intro x hx\n      use x, hx, 0, B₂.zero_mem\n      rw [add_zero]\n    · intro x hx\n      use 0, B₁.zero_mem, x, hx\n      rw [zero_add]\n  · intro x hx\n    rcases hx with ⟨b₁, hb₁, b₂, hb₂, rfl⟩\n    haveI : B₁ ≤ B₁ ⊔ B₂ := le_sup_left\n    replace hb₁ : b₁ ∈ B₁ ⊔ B₂ := this hb₁\n    haveI : B₂ ≤ B₁ ⊔ B₂ := le_sup_right\n    replace hb₂ : b₂ ∈ B₁ ⊔ B₂ := this hb₂\n    exact add_mem hb₁ hb₂\n\nend",
    "crumbs": [
      "09-subgroup.lean"
    ]
  },
  {
    "objectID": "09-subgroup.html#subgroups",
    "href": "09-subgroup.html#subgroups",
    "title": "09-subgroup.lean",
    "section": "4 Subgroups",
    "text": "4 Subgroups\n\n4.1 Objects\nThere’s nothing new about Subgroup G of a Group G compared to Subsemigroup and Submonoid. It just adds the closure under taking inverses.\nsection\n\nvariable (G : Type*) [Group G]\nvariable (H₁ H₂ : Subgroup G) (a b : G)\nexample : a ∈ H₁ → b ∈ H₁ → a * b ∈ H₁ := by apply mul_mem\nexample : (1 : G) ∈ H₁ := by apply one_mem\nexample : a ∈ H₁ → a⁻¹ ∈ H₁ := by apply inv_mem\nWe skip the lattice structure again.\nend\n\n\n4.2 Morphisms\nMonoidHom works for Subgroup as well.\nsection\n\nvariable {G₁ G₂ : Type*} [Group G₁] [Group G₂]\n         (f : G₁ →* G₂) (H₁ : Subgroup G₁) (H₂ : Subgroup G₂)\nImage and preimage of subgroups, upgraded to show closure under inverses.\n#check Subgroup.map\n#check Subgroup.comap\nFor groups, mker and mrange has been upgraded to ker and range respectively.\n#check MonoidHom.ker\n#check MonoidHom.range\n\nexample : MonoidHom.ker f = (⊥ : Subgroup G₂).comap f := by rfl\nexample : MonoidHom.ker f = {MonoidHom.mker f with\n      inv_mem' := by simp\n    } := by rfl\n\nend\n\n\n4.3 Normal Subgroups\nFor later discussions on quotient groups, we introduce normal subgroups here.\nsection\n\n#check Subgroup.Normal\nSubgroup.Normal is a bundled structure consisting of a proof of normality.\nexample {G : Type*} [Group G] (H : Subgroup G) :\n    H.Normal ↔ ∀ h ∈ H, ∀ g : G, g * h * g⁻¹ ∈ H := by\n  constructor\n  · intro ⟨h⟩\n    exact h\n  · intro h\n    exact ⟨h⟩\nThe kernel of a group homomorphism is a normal subgroup.\nexample {G₁ G₂ : Type*} [Group G₁] [Group G₂]\n    (f : G₁ →* G₂) : (f.ker).Normal := by\n  constructor\n  intro x hx y\n  rw [MonoidHom.mem_ker]\n  rw [map_mul, map_mul, hx, map_inv, mul_one, mul_inv_cancel]\nActually, Mathlib contains an instance for kernels, so that Lean automatically recognizes the normality of kernels.\n#check MonoidHom.normal_ker\nexample {G₁ G₂ : Type*} [Group G₁] [Group G₂]\n    (f : G₁ →* G₂) : (f.ker).Normal := inferInstance\n\nend\n[TODO]\n\nIndexed infimum and supremum of substructures\nxxxClass for substructures as canonical maps",
    "crumbs": [
      "09-subgroup.lean"
    ]
  },
  {
    "objectID": "03-logic.html",
    "href": "03-logic.html",
    "title": "03-logic.lean",
    "section": "",
    "text": "import Mathlib",
    "crumbs": [
      "03-logic.lean"
    ]
  },
  {
    "objectID": "03-logic.html#and-and-or",
    "href": "03-logic.html#and-and-or",
    "title": "03-logic.lean",
    "section": "1 And and Or",
    "text": "1 And and Or\nIn Lean’s dependent type theory, ∧ and ∨ serve as the direct product and the direct sum in the universe of Prop.\nEagle-eyed readers may notice that ∧ and ∨ act similarly to Cartesian product and disjoint union in set theory.\nThey are also constructed as inductive types.\nsection\n\nvariable (p q r : Prop)\n\n1.1 And (∧)\n\nIntroducing And\nThe only constructor of And is And.intro, which takes a proof of p and a proof of q to produce a proof of p ∧ q.\nIt is self-evident. Regard this as the universal property of the direct product if you like.\n#check And.intro\n\nexample (hp : p) (hq : q) : p ∧ q := And.intro hp hq\nAnd.intro hp hq can be abbreviated as ⟨hp, hq⟩, called the anonymous constructor.\nexample (hp : p) (hq : q) : p ∧ q := ⟨hp, hq⟩\nintroducing nested And\nexample (hp : p) (hq : q) (hr : r) : p ∧ q ∧ r := by\n  exact ⟨hp, hq, hr⟩ -- equivalent to `⟨hp, ⟨hq, hr⟩⟩`\nconstructor tactic applies And.intro to split the goal p ∧ q into subgoals p and q. You may also use the anonymous constructor notation ⟨hp, hq⟩ to mean And.intro hp hq.\nuse · to focus on the first goal in your goal list.\nexample (hp : p) (hq : q) : p ∧ q := by\n  constructor\n  · exact hp\n  · exact hq\non_goal tactic can be used to focus on a specific goal.\nexample (hp : p) (hq : q) : p ∧ q := by\n  constructor\n  on_goal 2 =&gt; exact hq\n  exact hp\nall_goals tactic can be used to simultaneously perform tactics on all goals.\nexample (hp : p) : p ∧ p := by\n  constructor\n  all_goals exact hp\nassumption tactic tries to close goals using existing hypotheses in the context. Can be useful when there are many goals.\nexample (hp : p) (hq : q) : p ∧ q := by\n  constructor\n  all_goals assumption\nsplit_ands tactic is like constructor but works for nested Ands.\nexample (hp : p) (hq : q) (hr : r) : p ∧ q ∧ r := by\n  split_ands\n  · exact hp\n  · exact hq\n  · exact hr\n[EXR] →–∨ distribution. Universal property of the direct product.\nexample (hrp : r → p) (hrq : r → q) : r → p ∧ q := by\n  intro hr\n  exact ⟨hrp hr, hrq hr⟩\n\n\nEliminating And\nAnd.left and And.right are among the elimination rules of And, which extract the proofs of p and q.\n#check And.left\n#check And.right\nexample (hpq : p ∧ q) : p := hpq.left\nexample (hpqr : p ∧ q ∧ r) : r := hpqr.right.right\nrcases hpq with ⟨hp, hq⟩ is a tactic that breaks down the hypothesis hpq : p ∧ q into hp : p and hq : q. Equivalently you can use have ⟨hp, hq⟩ := hpq.\nexample (hpq : p ∧ q) : p := by\n  rcases hpq with ⟨hp, _⟩\n  exact hp\nimplicit break-down in intro\nexample : p ∧ q → p := by\n  intro ⟨hp, _⟩\n  exact hp\nnested And elimination\nexample (hpqr : p ∧ q ∧ r) : r := by\n  rcases hpqr with ⟨_, _, hr⟩\n  exact hr\n[EXR] And is symmetric\nexample : p ∧ q → q ∧ p := by\n  intro ⟨hp, hq⟩\n  exact ⟨hq, hp⟩\n#check And.comm -- above has a name\n[EXR] →–∨ distribution, in another direction.\nexample (hrpq : r → p ∧ q) : (r → p) ∧ (r → q) := by\n  constructor\n  · intro hr\n    exact (hrpq hr).left\n  · intro hr\n    exact (hrpq hr).right\n\n\nCurrification\nThe actual universal elimination rule of And is the so-called decurrification: From (p → q → r) we may deduce (p ∧ q → r). This is actually a logical equivalence.\nIntuitively, requiring both p and q to deduce r is nothing but requiring p to deduce that q is sufficient to deduce r.\n[IGNORE] Decurrification is also self-evidently true in Lean’s dependent type theory.\nCurrification is heavily used in functional programming for its convenience, Lean is no exception.\nYou are no stranger to decurrification even if you are not a functional programmer: The universal property of the tensor product of modules says exactly the same. \\[\n\\operatorname{Hom}(M \\otimes N, P) \\cong \\operatorname{Hom}(M, \\operatorname{Hom}(N, P))\n\\]\n[EXR] currification\nexample (h : p ∧ q → r) : (p → q → r) := by\n  intro hp hq\n  exact h ⟨hp, hq⟩\n[EXR] decurrification\nexample (h : p → q → r) : (p ∧ q → r) := by\n  intro hpq\n  exact h hpq.left hpq.right\n\nexample (h : p → q → r) : (p ∧ q → r) := by\n  intro ⟨hp, hq⟩ -- `intro` is smart enough to destructure `And`\n  exact h hp hq\n\nexample (h : p → q → r) : (p ∧ q → r) := by\n  intro ⟨hp, hq⟩\n  apply h -- `apply` is smart enough to auto-decurrify and generate two subgoals\n  · exact hp\n  · exact hq\n[IGNORE] decurrification actually originates from And.rec, which is self-evident\n#check And.rec\ntheorem decurrify (h : p → q → r) : (p ∧ q → r) := And.rec h\nAnd.left is actually a consequence of decurrification\nexample : p ∧ q → p := by\n  apply decurrify\n  intro hp _\n  exact hp\n\n\n\n1.2 Iff (↔︎), first visit\nIt’s high time to introduce Iff here for the first time.\nIff (↔︎) contains two side of implications: Iff.mp and Iff.mpr.\nThough it is defined as a distinct inductive type, Iff may be seen as a bundled version of (p → q) ∧ (q → p). you may, somehow, even use it like a (p → q) ∧ (q → p). The only major difference is the name of the two components.\n#check Iff.intro\n#check Iff.mp\n#check Iff.mpr\n\nexample : (p ↔ q) ↔ (p → q) ∧ (q → p) := by\n  constructor\n  · intro h\n    exact ⟨h.mp, h.mpr⟩\n  · intro ⟨hpq, hqp⟩\n    exact ⟨hpq, hqp⟩\n\n\n1.3 Or (∨)\n\nIntroducing Or\nOr has two constructors, Or.inl and Or.inr. Either a proof of p or a proof of q produces a proof of p ∨ q.\n#check Or.inl\n#check Or.inr\n\nexample (hp : p) : p ∨ q := Or.inl hp\nleft (resp. right) tactic reduce Or goals to p (resp. q)\nexample (hq : q) : p ∨ q := by\n  right\n  exact hq\n\n\nEliminating Or\nTo prove r from p ∨ q, it suffices to prove both p → r and q → r. This is the elimination rule of Or, or the universal property of the direct sum.\n#check Or.elim\n#check Or.rec -- [IGNORE]\n\nexample (hpr : p → r) (hqr : q → r) : (p ∨ q → r) := fun hpq ↦ (Or.elim hpq hpr hqr)\nexample (hpr : p → r) (hqr : q → r) : (p ∨ q → r) := (Or.elim · hpr hqr) -- note the use of `·`\nexample (hpr : p → r) (hqr : q → r) (hpq : p ∨ q) : r := by\n  apply Or.elim hpq\n  · exact hpr\n  · exact hqr\nmatch-style syntax is designed to make use of Or.elim to destructure Or to cases. [IGNORE] You may just skim through this syntax for now.\nexample (hpr : p → r) (hqr : q → r) : (p ∨ q → r) := fun\n  | Or.inl hp =&gt; hpr hp\n  | Or.inr hq =&gt; hqr hq\nexample (hpr : p → r) (hqr : q → r) (hpq : p ∨ q) : r :=\n  match hpq with\n  | Or.inl hp =&gt; hpr hp\n  | Or.inr hq =&gt; hqr hq\nexample (hpr : p → r) (hqr : q → r) (hpq : p ∨ q) : r := by\n  match hpq with\n  | Or.inl hp =&gt; exact hpr hp\n  | Or.inr hq =&gt; exact hqr hq\nexample (hpr : p → r) (hqr : q → r) (hpq : p ∨ q) : r := by\n  cases hpq with\n  | inl hp =&gt; exact hpr hp\n  | inr hq =&gt; exact hqr hq\nrcases may also serve as a tactic version of match, which is much more convenient.\nexample (hpr : p → r) (hqr : q → r) (hpq : p ∨ q) : r := by\n  rcases hpq with (hp | hq) -- `rcases` can also destructure `Or`\n  · exact hpr hp\n  · exact hqr hq\nexample (hpr : p → r) (hqr : q → r) : p ∨ q → r := by\n  rintro (hp | hq) -- `rintro` is a combination of `intro` and `rcases`\n  · exact hpr hp\n  · exact hqr hq\n[EXR] distributive laws\nexample : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by sorry\nexample : p ∨ (q ∧ r) ↔ (p ∨ q) ∧ (p ∨ r) := by sorry\n\nend",
    "crumbs": [
      "03-logic.lean"
    ]
  },
  {
    "objectID": "03-logic.html#forall-and-exists",
    "href": "03-logic.html#forall-and-exists",
    "title": "03-logic.lean",
    "section": "2 Forall and Exists",
    "text": "2 Forall and Exists\n\n2.1 Forall (∀)\nAs you may have already noticed, ∀ is just an alternative way of writing →. Say p is a predicate on a type X, i.e. of type X → Prop, then ∀ x : X, p x is exactly the same as (x : X) → p x.\nThough → is primitive in Lean’s dependent type theory, we may still (perhaps awkwardly) state the introduction and elimination rules of ∀:\n\nIntroduction: fun (x : X) ↦ (h x : p x) produces a proof of ∀ x : X, p x.\nElimination: Given a proof h of ∀ x : X, p x, we can obtain a proof of p a for any specific a : X. It is exactly h a.\n\nsection\n\nvariable {X : Type} (p q : X → Prop) (r s : Prop) (a b : X)\n\n#check ∀ x : X, p x\n#check ∀ x, p x -- Lean is smart enough to infer the type of `x`\n\nexample : (∀ x : X, p x) → p a := by\n  intro h\n  exact h a\n[IGNORE] Writing ∀ emphasizes that the arrow → is of dependent type, and the domain X is a type, not a proposition. But they are just purely psychological, as the following examples show.\nexample : (hrs : r → s) → (∀ _ : r, s) := by\n  intro hrs\n  exact hrs\n\n\n2.2 Exists (∃)\n∃ is a bit more complicated.\nSlogan: ∀ is a dependent →, ∃ is a dependent × (or ∧ in Prop universe)\n#check ∃ x : X, p x\n#check ∃ x, p x -- Lean is smart enough to infer the type of `x`\n\nIntroducting Exists\n∃ x : X, p x means that we have the following data:\n\nan element a : X;\na proof h : p a.\n\nSo a pair (a, h) would suffice to construct a proof of ∃ x : X, p x.\nThis is the defining introduction rule of Exists as an inductive type.\n#check Exists.intro\nexample (a : X) (h : p a) : ∃ x, p x := Exists.intro a h\nAs like And, you may use the anonymous constructor notation ⟨a, h⟩ to mean Exists.intro a h.\nexample (a : X) (h : p a) : ∃ x, p x := ⟨a, h⟩\nIn tactic mode, use a make use of Exists.intro a to reduce the goal ∃ x : X, p x to p a.\nexample (a : X) (h : p a) : ∃ x, p x := by use a\n\n-- [EXR]\nexample (x y z : ℕ) (hxy : x &lt; y) (hyz : y &lt; z) : ∃ w, x &lt; w ∧ w &lt; z :=\n  ⟨y, ⟨hxy, hyz⟩⟩\nNote that in the defining pair (a, h), h is a proof of p a, whose type depends on a. Thus psychologically, you may view ∃ x : X, p x as a dependent pair type (x : X) × (p x).\nHave writing Exists as a dependent pair type reminded you of the currification process?\n\n\nEliminating Exists\nTo construct the implication (∃ x : X, p x) → q, it suffices to have a proof of (∀ x : X, p x → q), i.e. (x : X) → p x → q. Exists.elim does exactly above.\n#check Exists.elim\n\nexample : (∀ x, p x → r) → ((∃ x, p x) → r) := by\n  intro hf he\n  exact Exists.elim he hf\nIn tactic mode, rcases h with ⟨a, ha⟩ make use of this elimination rule to break down a hypothesis h : ∃ x : X, p x into a witness a : X and a proof ha : p a.\nexample : (∀ x, p x → r) → ((∃ x, p x) → r) := by\n  intro hf he\n  rcases he with ⟨a, hpa⟩\n  exact hf a hpa\n\nexample : (∀ x, p x → r) → ((∃ x, p x) → r) := by\n  intro h ⟨a, hpa⟩ -- you may also `rcases` explicitly\n  exact h a hpa\n[EXR] reverse direction is also true\nexample :  ((∃ x, p x) → r) → (∀ x, p x → r) := by\n  intro h a hpa\n  apply h\n  use a\n\n-- [EXR]\nexample : (∃ x, r ∧ p x) → r ∧ (∃ x, r ∧ p x) := by\n  intro ⟨a, ⟨hr, hpa⟩⟩\n  exact ⟨hr, ⟨a, ⟨hr, hpa⟩⟩⟩\n\n-- [EXR]\nexample : (∃ x, p x ∨ q x) ↔ (∃ x, p x) ∨ (∃ x, q x) := by\n  constructor\n  · rintro ⟨a, (hpa | hqa)⟩\n    · left; use a\n    · right; use a\n  · rintro (⟨a, hpa⟩ | ⟨a, hqa⟩)\n    · use a; left; exact hpa\n    · use a; right; exact hqa\n\nend\n\n\n[IGNORE] A cosmological remark\nThe pair (a, h) actually do not have type (x : X) × (p x). The latter notation is actually for the dependent pair type (or Sigma type), which lives in Type* universe.\nBut Exists should live in Prop, and in Prop universe we admit proof-irrelevance, i.e. we do not save data. So Exists forget the exact witness a once it is proved.\nThis “forgetfulness” is revealed by the fact that there is no elimination rule Exists.fst to extract the witness a from a proof of ∃ x : X, p x, as long as X lives in the Type* universe. (Note that Exists.elim can only produce propositions in Prop)\nBut if X lives in Prop universe, then we do have Exists.fst:\nsection\n\n#check Exists.fst\nWait, wait, we never worked with X : Prop before. Say p : r → Prop and r s : Prop, what does ∃ hr : r, p hr mean? It means that r and p hr are both true? [TODO] I don’t know how to explain this properly so far.\nvariable (r : Prop) (p : r → Prop)\n#check ∃ hr : r, p hr\n\n-- Prove `Exists.fst` and `Exists.snd` by `Exists.elim`\nexample (he : ∃ hr : r, p hr) : r ∧ p he.fst := by\n  apply Exists.elim he\n  intro hr hpr\n  exact ⟨hr, hpr⟩\n\nend",
    "crumbs": [
      "03-logic.lean"
    ]
  },
  {
    "objectID": "03-logic.html#ignore-a-cosmological-remark-continued",
    "href": "03-logic.html#ignore-a-cosmological-remark-continued",
    "title": "03-logic.lean",
    "section": "3 [IGNORE] A cosmological remark, continued",
    "text": "3 [IGNORE] A cosmological remark, continued\nSame construction, different universes. Other examples are also shown below.\n#print And -- `×` in `Prop`\n#print Prod -- `×` in `Type*`\n\n-- Forall `∀`: dependent `∏` in `Prop`\n-- dependent function type: dependent `∏` in `Type*`\n\n#print Or -- `⊕` in `Prop`\n#print Sum -- `⊕` in `Type*`\n\n#print Exists -- dependent `∑` in `Prop`\n#print Sigma -- dependent `Σ` in `Type*`\n\n#print Nonempty -- a proof of non-emptiness living in `Prop`\n#print Inhabited -- an designated element living in `Sort*`",
    "crumbs": [
      "03-logic.lean"
    ]
  },
  {
    "objectID": "03-logic.html#remainder",
    "href": "03-logic.html#remainder",
    "title": "03-logic.lean",
    "section": "4 Remainder",
    "text": "4 Remainder\nIff (↔︎), second visit, bundled and unbundled version\nwe do it with Eq? Eq is hard. Maybe a second visit when touching inductive types.",
    "crumbs": [
      "03-logic.lean"
    ]
  },
  {
    "objectID": "02-logic.html",
    "href": "02-logic.html",
    "title": "02-logic.lean",
    "section": "",
    "text": "import Mathlib\nYou may skip the materials tagged with [IGNORE] for the first runthrough. Most of them are here to illustrate the nature of inductive types, which may be too advanced for beginners.\nMaterials tagged with [EXR] are recommended for you to try before looking at the solution.\nMaterials tagged with [TODO] means that I’m still working on it, or I’m not sure about the content yet. Feel free to give your suggestions!",
    "crumbs": [
      "02-logic.lean"
    ]
  },
  {
    "objectID": "02-logic.html#and",
    "href": "02-logic.html#and",
    "title": "02-logic.lean",
    "section": "1 : and :=",
    "text": "1 : and :=\n3 : ℕ means that 3 is a term of type ℕ.\nBy the Curry–Howard correspondence, hp : p means that hp is a proof of the proposition p.\n#check 3\n#check ℕ\n\n#check ∀ x : ℝ, 0 ≤ x ^ 2\n#check sq_nonneg\n#check (sq_nonneg : ∀ x : ℝ, 0 ≤ x ^ 2)\n:= is used to define terms.\ndef myThree : ℕ := 3\n\n#check myThree\ntheorem is just a definition in the Prop universe By the Curry–Howard correspondence, for theorem, behind :, the theorem statement follows; behind :=, a proof should be given.\ntheorem thm_sq_nonneg : ∀ x : ℝ, 0 ≤ x ^ 2 := sq_nonneg\n\n-- `example` is just an anonymous theorem\nexample : ∀ x : ℝ, 0 ≤ x ^ 2 := thm_sq_nonneg",
    "crumbs": [
      "02-logic.lean"
    ]
  },
  {
    "objectID": "02-logic.html#implication",
    "href": "02-logic.html#implication",
    "title": "02-logic.lean",
    "section": "1 Implication →",
    "text": "1 Implication →\nImplication → is the most fundamental way of constructing new types in Lean’s dependent type theory. It’s one of the first-class citizens in Lean.\nIn the universe of Prop, for propositions p and q, the implication p → q means “if p then q”.\nsection\n\nvariable (p q r : Prop) -- this introduces global variables within this section\n\n#check p\n#check q\n#check p → q\n→ is right-associative. In general, hover the mouse over the operators to see how they associate. so p → q → r means p → (q → r). You may notice that this is logically equivalent to p ∧ q → r. This relationship is known as currification. We shall discuss this later.\nmodus ponens\ntheorem mp : p → (p → q) → q := by sorry -- `sorry` is a placeholder for unfinished proofs\nBy the Curry–Howard correspondence, p → q is also understood as a function that takes a proof of p and produces a proof of q.\nWe introduce an important syntax to define functions / theorems: When we define a theorem theorem name (h1 : p1) ... (hn : pn) : q := ..., we are actually defining a function name of type (h1 : p1) → ... → (hn : pn) → q. Programmingly, h1, …, hn are the parameters of the function and q is the return type.\nThe significance of this syntax, compared to theorem name : p1 → ... → pn → q := ..., is that now h1, …, hn, proofs of p1, …, pn, are now introduced as hypotheses into the context, available for you along the way to prove q.\nthis proves a theorem of type p → p\nexample (hp : p) : p := hp\nmodus ponens, with a proof\nexample (hp : p) (hpq : p → q) : q := hpq hp\nA function can also be defined inline, using fun (lambda syntax): fun (h1 : p1) ... (hn : pn) ↦ (hq : q) defines a function of type (h1 : p1) → ... → (hn : pn) → q\nSome of the type specifications may be omitted, as Lean can infer them.\nexample : p → p := fun (hp : p) ↦ (hp : p)\nexample : p → p := fun (hp : p) ↦ hp\nexample : p → (p → q) → q := fun (hp : p) (hpq : p → q) ↦ hpq hp\nexample : p → (p → q) → q := fun hp hpq ↦ hpq hp",
    "crumbs": [
      "02-logic.lean"
    ]
  },
  {
    "objectID": "02-logic.html#tactic-mode",
    "href": "02-logic.html#tactic-mode",
    "title": "02-logic.lean",
    "section": "2 Tactic Mode",
    "text": "2 Tactic Mode\nConstruct proofs using explicit terms is called term-style proof. This can be tedious for complicated proofs.\nFortunately, Lean provides the tactic mode to help us construct proofs interactively.\nby activates the tactic mode.\nThe tactic mode captures the way mathematicians actually think: There is a goal q to prove, and we have several hypotheses h1 : p1, …, hn : pn in the context to use. We apply tactics to change the goal and the context until the goal is solved. This produces a proof of p1 → ... → pn → q.\nexample (hp : p) : p := by exact hp\ntactic: exact If the goal is p and we have hp : p, then exact hp solves the goal.\nexact? may help to close some trivial goals\nexample (hp : p) (hpq : p → q) : q := by exact?\ntactic: intro Sometimes a hypothesis is hidden in the goal in the form of an implication. If the goal is p → q, then intro hp changes the goal to q and adds the hypothesis hp : p into the context.\nmodus ponens, with a hidden hypothesis\nexample (hp : p) : (p → q) → q := by\n  intro hpq\n  exact hpq hp\n\nexample (hq : q) : p → q := by\n  intro _  -- use `_` as a placeholder if the introduced hypothesis is not needed\n  exact hq\nmodus ponens, with two hidden hypothesis\nexample : p → (p → q) → q := by\n  intro hp hpq -- you can `intro` multiple hypotheses at once\n  exact hpq hp\n[EXR] transitivity of →\nexample : (p → q) → (q → r) → (p → r) := by\n  intro hpq hqr hp\n  exact hqr (hpq hp)\ntactic: apply If q is the goal and we have hpq : p → q, then apply hpq changes the goal to p.\nmodus ponens\nexample (hp : p) (hpq : p → q) : q := by\n  apply hpq\n  exact hp\n[EXR] transitivity of →\nexample (hpq : p → q) (hqr : q → r) : p → r := by\n  intro hp\n  apply hqr\n  apply hpq\n  exact hp\n[IGNORE] Above tactics are minimal and sufficient for simple proofs. When proofs went more complicated, you may want more tactics that suit your needs. Remember your favorite tactics and use them accordingly.\ntactic: specialize If we have hpq : p → q and hp : p, then specialize hpq hp reassigns hpq to hpq hp, a proof of q.\nexample (hp : p) (hpq : p → q) : q := by\n  specialize hpq hp\n  exact hpq\n\nexample (hpq : p → q) (hqr : q → r) : p → r := by\n  intro hp\n  specialize hpq hp\n  specialize hqr hpq\n  exact hqr\ntactic: have have helps you to state and prove a lemma in the middle of a proof. have h : p := hp adds the hypothesis h : p into the context, where hp is a proof of p that you provide.\nhaveI is similar to have, but it adds the hypothesis as this.\nexample (hpq : p → q) (hqr : q → r) : p → r := by\n  intro hp\n  have hq : q := hpq hp\n  have hr : r := by -- combine with `by` is also possible\n    apply hqr\n    exact hq\n  exact hr\ntactic: suffices Say our goal is q, suffices hp : p from hq changes the goal to p, as long as you can provide a proof hq of q from a proof hp of p. You may also switch to the tactic mode by suffices hp : p by ...\nexample (hpq : p → q) (hqr : q → r) : p → r := by\n  intro hp\n  suffices hq : q from hqr hq\n  exact hpq hp\n\nexample (hpq : p → q) (hqr : q → r) : p → r := by\n  intro hp\n  suffices hq : q by\n    apply hqr\n    exact hq\n  exact hpq hp\nshow (it is not a tactic!) Sometimes you want to clarify what exactly you are giving a proof for. show p from h make sure that h is interpreted as a proof of p. show p by ... switches to the tactic mode to construct a proof of p.\nexample (hpq : p → q) (hqr : q → r) : p → r := by\n  intro hp\n  exact hqr (show q by apply hpq; exact hp)\n\nend",
    "crumbs": [
      "02-logic.lean"
    ]
  }
]